{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ec9bcf3-9d20-46a6-99b4-942095e5ae2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Project Phase 3 \n",
    "\n",
    "## Final Call: Near-Real-Time Flight Delay Prediction at Scale\n",
    "\n",
    "Team: Alex Lewis, Missael Isaac Vasquez, Rohan Krishnamurthi, Shyam Patel, Matthew Paterno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a4b708d-8122-40aa-8740-e4cdeac542f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "\n",
    "## Phase Leader Plan\n",
    "\n",
    "| Week | Team Member           |\n",
    "|-----------------------------|-----------------------|\n",
    "| Week 1                      | Rohan Krishnamurthi   |\n",
    "| Week 2                      | Missael Vasquez       |\n",
    "| Week 3                      | Matthew Paterno       |\n",
    "| Week 4                      | Shyam Patel           |\n",
    "| Week 5                      | Alex Lewis            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8cd4a28-2dc0-431d-9224-09c602a376c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Credit Assignment Plan\n",
    "#### SMART Goals\n",
    "\n",
    "| Tasks | Tasks Owners | Deliverable | Due Date | Time Required | Relevancy to Final Deliverable | Comments |\n",
    "|-------|--------------|-------------|----------|---------------|-------------------------------|----------|\n",
    "|Weather Prediction Model Updates| Alex Lewis | Improved Adverse Weather Prediction Feature | 7/30 | 3 hours | Steps towards creating an accurate weather prediction feature | Predictions could be further tuned, but want to attempt to incorporate feature into pipeline first |\n",
    "|Adjust Weather Variable Processing| Alex Lewis | Change Course to New Set of Weather Features | 8/1 | 2 hours | Alternative solution to incorporating weather conditions in final models | This approach allows a simpler solution to using weather information without leakage |\n",
    "|Create Self-Contained Weather Feature Functions| Alex Lewis | Set of Functions to be Included in Feature Engineering Pipeline | 8/3 | 2 hours | Build weather features in self-contained functions to be used in final feature engineering solution | Functions should be easily incorporated into pipeline |\n",
    "|Create Feature Engineering - Weather Slides| Alex Lewis | Slides for Final Presentation | 8/6 | 1 hour | Slides with information about relevance of weather and how features were created | Includes additional research into percentage of weather delays outside of our dataset |\n",
    "|Create/Fine-Tune Logistic Regression Model| Alex Lewis | Logistic Regression Model | 8/8 | 3 hours | Can serve as baseline before more complex modeling | Tried to maintain preprocessing with additional tuning |\n",
    "|Edit/Produce Write-Ups for Report| Alex Lewis | Write-Ups for Weather and Logistic Regression | 8/8 | 2 hours | Completed initial write-ups for weather and logistic regression sections of the final report | Will need to incorporate with other feature engineering and modeling sections |\n",
    "| XGBoost regression modeling | Matthew Paterno | Trained SparkXGBRegressor on imputed numerics + string-indexed categoricals, assembled via VectorAssembler; optional signed log1p target transformation | 8/8 | 3 hrs | Core to delay prediction objective | objective=reg:squarederror, tree_method=hist, subsample=0.8, colsample_bytree=0.8 |\n",
    "| Capping | Matthew Paterno | DEP_DELAY_CAPPED column with values winsorized at symmetric abs 99th percentile | 8/8 | 0.5 hrs | Reduces skew and stabilizes model fit | q99 computed from training set only to avoid data leakage |\n",
    "| Baseline | Matthew Paterno | RMSE, MAE, RÂ² for zero, mean, and median constant predictors on train/test sets | 8/8 | 0.5 hrs | Establishes performance floor |  |\n",
    "| Time-series CV | Matthew Paterno | Cross-validation results with mean RMSE and variability across 5 chronological folds | 8/8 | 2 hrs | Prevents temporal leakage in validation |  |\n",
    "| Hyperparameter optimization | Matthew Paterno | Best fold-averaged RMSE and parameter set from grid search over max_depth, min_child_weight, and learning_rate | 8/8 | 3 hrs | Improves generalization and model fit | n_estimators=1000, subsample=0.8, colsample_bytree=0.8, early stopping=30 |\n",
    "|KDE plot EDA| Rohan Krishnamurthi | Visual & analysis | 7/27 | 1 hour | Assist overall comprehension of variation present in the different time framed datasets | Delays are mostly centered aroud 0 which is great and means on average we have no delays! |\n",
    "|Dataset table analyses| Rohan Krishnamurthi | Tables and analysis | 7/27 | 30 mins | Identify differences across the different datasets | results and analyses in tables below |\n",
    "|Abstract| Rohan Krishnamurthi | Written abstract | 8/9 | 1 hour | Align project goals with process plan and provide a hollistic overview of our project | FP1 feedback taken into consideration for all aspects of the new abstract |\n",
    "|Elastic Net Linear Regression| Rohan Krishnamurthi | ML Model & Results | 7/27 | 1 hour | Explore elastic net linear regression as a viable machine learning algorithm | Linear modeling is not advanced enough to capture the intricate relationships in our data, this model will not be explored further |\n",
    "|Random Forest Model regression & Classification| Rohan Krishnamurthi | ML Model & Results | 8/8 | 6 hours | Explore random forest as a viable machine learning algorithm | great iniital results and very helpful to guide overall modeling process. was not selected as a great algorithm due to better performances seen from other models |\n",
    "|XGBoost Model| Rohan Krishnamurthi | ML Model & Results | 8/8 | 2 hours | Explore XGBoost as a viable machine learning algorithm | Simple eploration to understand how model would perform, led us to identify catboost as a viable solution which ended up being a great selection |\n",
    "|CatBoost Regressor and Classifier| Rohan Krishnamurthi | ML Model & Results | 8/8 | 6 hours | Explore CatBoost as a viable machine learning algorithm | Great model performance as this algorothm handles categorical features extremely well and yielded meaningful results for various business use cases |\n",
    "|MLP Neural Network Regressor| Rohan Krishnamurthi | ML Model & Results | 8/8 | 3 hours | Explore MLP NN as a viable machine learning algorithm | Advanced modelling yielded great results for our intricate dataset and configuration, exhaustive model but best results overall |\n",
    "|Evaluation metrics| Rohan Krishnamurthi |Metric selection | 8/8 | 1 hour | idenntify optimal metrics to provide robust evaluation of model performances | regression and classification metrics established to provide consistency throughout modeling process |\n",
    "|Advanced work (extra credit)| Rohan Krishnamurthi | extra credit | 8/8 | 4 hours | 2020 & 2021 predictions, and deep learning techniques | predictions for simulated data in 2020 and 2021 were conducted. as well as a deep learning NN with keras and tensorflow which yielded great results |\n",
    "|Feature Engineering Pipeline Integration/Debugging/Optimization| Shyam Patel | Faster, more complete, and robust data prep pipeline | 8/2 | 8 hours | Keep model inputs up to date with new engineered features from teammates and ensure a consistent source of truth while minimizing computational load | Works as intended, has been reducing overhead on new model experiments as data comes with the desired features |\n",
    "|XGboost Experimentation| Shyam Patel | Insights around most important features and their contributions to the model | 8/4 | 10 hours | Allows the team to determine which features are the most predictive and increases understanding of the problem space |Has guided feature development through the project. |\n",
    "|Feature Development| Shyam Patel | Expanded features around flight timing | 8/2 | 8 hours | Captures more of the propagation effect of delays from surrounding flights and expands on what is clearly our best set of features from the first iteration. | Has boosted R^2 further, implying that models are able to explain more of the variations in delays. |\n",
    "|MLP Classifier (Extra Credit)| Shyam Patel | Exploratory results around MLP approach | 8/9 | 2 hours | A look into a more sophisticated model architecture that could boost results. | Has given us an excellent F1 although the AUC metrics imply overall performance is very threshold specific. |\n",
    "|Feature Engineering: Additional Flight Features| Missael Vasquez | A deeper look and developed of flight features | 8/4 | 8 hours | Was able to develop two new features, diversion & cancellation flight amount within a 2-6 hour window prior to a flight's departure. | Provided a more nuanced representation of chain impacts of diversion/cancellation, in the immediacy of a 2-6 hour prior to a flight's arrival. \n",
    "|XBGBoost Classifier | Missael Vasquez | Developed XGBoost Binomial Delay Classifer and Classification Framework | 8/3 | 8 hours | Tried to disseminate the practicality of a binomial classification of delay | Produced excellent understanding of the various techniques and frameworks necessary to develop reliable and consistent results, such as adjustment in thresholding and re distributed weighing for more stable results; lengthy hyperparameter turning to produce best such results. |\n",
    "|MLP Exploration for Improvement| Missael Vasquez | Tested and tried but ultimately failed in idenitfying new transformations, weights, or optimal tuning parameters to substantially move the needle in improving our MLP results. | 8/5-8/9 | 10 hours | Quickly understood the difficulty in attempting to produce meaningful improvements to our model. |  Consistently, able to repoduce MAE in the 19-24 range, but never anything to close the 15 minute gap found. Hypertuning proved to be difficult and quite unsustainable approach to farming improvement, as these models were exhaustive and lengthy to run in our current systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f21fdbc-be7b-45ca-a2dd-5b6c73fedf30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Team Members\n",
    "| Team Member        | Email | Image |\n",
    "|--------------------|-------|-------|\n",
    "| Rohan Krishnamurthi | rohankrish223@berkeley.edu | <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/Portraits/rohanKrishnamurthi.jpg\" alt=\"Rohan\" width=\"80\"/>  |\n",
    "| Missael Vasquez    | mvasquez1995@berkeley.edu | <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/Portraits/mv_headshot.jpg\" alt=\"Missael\" width=\"80\"/>      |\n",
    "| Matthew Paterno    | mpaterno@berkeley.edu | <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/Portraits/matt_paterno.jpg\" alt=\"Matt\" width=\"80\"/> |\n",
    "| Shyam Patel        | shyam_patel@berkeley.edu | <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/Portraits/Shyam_Headshot.jpeg\" alt=\"Shyam\" width=\"80\"/> |\n",
    "| Alex Lewis         | apl@berkeley.edu | <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/Portraits/AlexLewis.jpg\" alt=\"Alex\" width=\"80\"/> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e4b451e-8f3c-4d8a-9ca6-764dfae541ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "\n",
    "Flight delays are a consistent challenge in U.S. air travel, disrupting passenger itineraries and reducing airline operational efficiency. This project develops predictive and classification models to estimate the magnitude of departure delays for American domestic flights, leveraging large-scale datsets from the U.S. Department of Transportation (DOT) for flight operations and the National Oceanic and Atmospheric Administration (NOAA) for historical weather records, with data spanning 2015-2019. The prediction task is limited to information available two hours prior to a given flight's scheduled departure, to ensure applicability in real world scenarios. Delays are defined as departures 15 minutes or more behind schedule, with predictions focusing on estimating delay as a continuous target variable as well as classification of major delay events. \n",
    "\n",
    "Using PySpark on Databricks, we conducted exploratory data analysis (EDA) to identify temporal, spatial, and operational trends in delay patterns. Using these insights, we engineered targeted features spanning weather conditions, carrier performance, traffic patterns, and network connectivity metrics. All engineered features were constrained to the 2 hour prediction window, and time dependent aggregates were calculated based on historical information to preserve temporal relationships. \n",
    "\n",
    "We evaluated a wide range of modeling approaches, including logistic & linear regression, elastic net, random forest, XGBoost, CatBoost, ensemble frameworks, and multi-layer perceptron (MLP) neural networks. Ensemble methods and neural networks consistently yielded the strongest performance. These models were trained and validated using a time series cross validation, with a final evaluation conducted on the year 2019 to avoid any temporal violations. Both classification and regression approaches were chosen to support one another and provide independent results if either was more desirable based on the business use case. Future work could include leveraging the results of the classification model to feed into the regression model, which we anticipate will enhance our model performance greatly. \n",
    "\n",
    "Performance was assessed using multiple metrics: mean absolute error (MAE) for intepretability, root mean squared error (RMSE) for penalizing large errors, and R squared in a limited fashion for variance explanation. Classification models also used appropriate metrics such as recall, F1 score, precision, and ROC/PR area under curve (AUC). This combination provided a holistic view of predictive quality, avoiding possible misinterpretation from a single metric evaluation. Preliminary results indicate that the extensive feature engineering meaningfully improves overall model performance. \n",
    "\n",
    "By integrating scalable big data-processing with carefully engineered models, our work demonstrates the potential for near real-time delay forecasting, offering actionable insights to improve passenger planning and airline operational decision making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99185b14-3981-4a4c-a6a7-7c7eeacb8c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Evaluation Metrics\n",
    "\n",
    "#### Regression\n",
    "\n",
    "Mean absolute error (MAE): measures the average magnitude of errors between predicted and actual values, without considering direction, meaning that positive and negative errors are treated equally due to the absolute value aspect of this metric. It is more robust to outliers than metrics which square errors, such as RMSE, making it a great compliment to metrics of that nature so we can provide a holistic evaluation of our models. It is easily interpretable since the result is in the same units as the predicted variable dep_delay (minutes) and can be use across a variety of different models for facilitated comparison. \n",
    "\n",
    "**Formula**\n",
    "$$\n",
    "\\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left|y_i - \\hat{y}_i\\right|\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "$$n = \\text{number of observations}$$  \n",
    "$$y_i = \\text{actual value for the $i^{th}$ observation}$$  \n",
    "$$\\hat{y}_i = \\text{predicted value for the $i^{th}$ observation}$$\n",
    "\n",
    "Root mean squared error (RMSE): measures the square root of the average of squared differences between predicted and actual values. This metric penalizes large errors more heavily because the errors are squared before averaging, making it sensitive to outliers. For this reason, it can be useful in specific problems such as this where we want to mark large delays as especially detrimental. \n",
    "\n",
    "**Formula**\n",
    "$$\n",
    "\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "$$n = \\text{number of observations}$$  \n",
    "$$y_i = \\text{actual value for the $i^{th}$ observation}$$  \n",
    "$$\\hat{y}_i = \\text{predicted value for the $i^{th}$ observation}$$\n",
    "\n",
    "R-squared: measures the proportion of variance in the dependent variable that is predictable from the independent variables. It evaluates how well the model's predictions fit the actual data, and ranges from 0 to 1 with 1 being a perfect fit. This metric was used sparingly since it is best suited for linear models, but nonetheless, helped create a complete picture for proper evaluation in some cases. \n",
    "\n",
    "**Formula**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "$$n = \\text{number of observations}$$  \n",
    "$$y_i = \\text{actual value for the $i^{th}$ observation}$$  \n",
    "$$\\hat{y}_i = \\text{predicted value for the $i^{th}$ observation}$$  \n",
    "$$\\bar{y} = \\text{mean of the actual values}$$\n",
    "\n",
    "#### Classification\n",
    "\n",
    "Accuracy: measures the proportion of total predictions that the model correctly classifies. It is easily understood and intuitive, making it a great classification metric that can be widely applicable. \n",
    "\n",
    "**Formula**\n",
    "$$\n",
    "\\mathrm{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "$$TP = \\text{True Positives}$$  \n",
    "$$TN = \\text{True Negatives}$$  \n",
    "$$FP = \\text{False Positives}$$  \n",
    "$$FN = \\text{False Negatives}$$\n",
    "\n",
    "Precision: measures the proportion of positive predictions that are actually correct. It answers the question of, when the model predicts positive, how many are actually correct? It is quite useful for this context when false positives can be costly, such as predicting a delay when a given flight is actually on time. \n",
    "\n",
    "**Formula**\n",
    "$$\n",
    "\\mathrm{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "$$TP = \\text{True Positives}$$  \n",
    "$$FP = \\text{False Positives}$$\n",
    "\n",
    "F1 score: the harmonic mean of precision and recall, balancing the two metrics. By summarizing two metrics in a single measure, F1 is robust to imbalance and provides a complete picture when neither precision or recall fully can. \n",
    "\n",
    "**Formula**\n",
    "$$\n",
    "\\mathrm{F1} = 2 \\times \\frac{\\mathrm{Precision} \\times \\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}}\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "$$\\mathrm{Precision} = \\frac{TP}{TP + FP}$$  \n",
    "$$\\mathrm{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Area Under ROC Curve (AUC): measures a model's ability to distinguish between positive and negative classes across all classification thresholds. This is a powerful, threshold-independent metric that is insensitive to class imbalance and reflects how well the model ranks positive instances above negative ones. \n",
    "\n",
    "**Formula**\n",
    "AUC is typically computed numerically, but its definition is:\n",
    "$$\n",
    "\\mathrm{AUC} = \\int_{0}^{1} \\mathrm{TPR}(t) \\, d(\\mathrm{FPR}(t))\n",
    "$$\n",
    "\n",
    "**Where:**  \n",
    "$$\\mathrm{TPR} = \\frac{TP}{TP + FN} \\quad \\text{(Recall)}$$  \n",
    "$$\\mathrm{FPR} = \\frac{FP}{FP + TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfcb2c9f-03b5-4fe9-aa72-33723fa6dd54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We conducted EDA on the On-Time Performance with Weather (OTPW) dataset across 3-month, 1-year, and 5-year windows to assess distributions, missingness, and feature characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac87c8aa-88d2-4bc0-a2ea-291c9375f531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Addressing duplicates\n",
    "\n",
    "| Dataset | Duplicate Count          |\n",
    "|-----------------------------|-----------------------|\n",
    "| OTPW 3 months                    | 55364  |\n",
    "| Flights 3 months                    | 1403471       |\n",
    "| OTPW 12 months                  | 5811854       |\n",
    "| Flights 12 months                    | 7422037          |\n",
    "| OTPW 60 months               | 0           |\n",
    "| Flights 60 months               | 31746841           |\n",
    "\n",
    "At a high level, flight data can be duplicative in nature due to the re-entry of data using the same tail number upon arrival and departure. While humans understand the difference, depending on the joins used and data extraction, this can be seen as a duplicate given the same tail number is present in two or more rows, with similar accompanying flight data. High duplicates in the flight data confirm this suspicion, and the diminished duplicate quantity in OTPW suggests there was handling to address this issue through the join that created the OTPW dataset. The major outlier (in a good way) here is the full OTPW dataset which contains 0 duplicates. This suggests that a dedeuplication step was run after the join, maybe using dropDuplicates() on key identification columns (`FL_DATE`, `TAIL_NUM`, `ORIGIN`, etc.). Or the join logic was completely refined for the full 60 month dataset here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1f01ca3-e20e-4746-a99d-0f4ab235aac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39deddf-c4c8-4410-80bd-4df9f52ccf50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "| Variable                                  |   Nulls (3mo) |   Nulls (1yr) |      Nulls (5yr) |\n",
    "|:---------------------------------------------|--------------:|--------------:|-----------------:|\n",
    "| ACTUAL_ELAPSED_TIME                          |         46602 |        225099 | 566380           |\n",
    "| AIR_TIME                                     |         46602 |        225099 | 566380           |\n",
    "| ARR_DEL15                                    |         46602 |        225099 | 568978           |\n",
    "| ARR_DELAY                                    |         46602 |        225099 | 568978           |\n",
    "| ARR_DELAY_GROUP                              |         46602 |        225099 | 568978           |\n",
    "| ARR_DELAY_NEW                                |         46602 |        225099 | 568978           |\n",
    "| ARR_TIME                                     |         44317 |        187549 | 500369           |\n",
    "| ARR_TIME_BLK                                 |             0 |             0 |      0           |\n",
    "| AWND                                         |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| BackupDirection                              |        898903 |      11070265 |      1.96462e+07 |\n",
    "| BackupDistance                               |        898903 |      11070265 |      1.96462e+07 |\n",
    "| BackupDistanceUnit                           |        898903 |      11070265 |      1.96462e+07 |\n",
    "| BackupElements                               |        861168 |      10576146 |      1.84144e+07 |\n",
    "| BackupElevation                              |       1035307 |      12789308 |      2.30855e+07 |\n",
    "| BackupEquipment                              |        914026 |      11236952 |      1.98991e+07 |\n",
    "| BackupLatitude                               |       1017801 |      12543317 |      2.25838e+07 |\n",
    "| BackupLongitude                              |       1017801 |      12543317 |      2.25838e+07 |\n",
    "| BackupName                                   |        840001 |      10347163 |      1.80212e+07 |\n",
    "| CANCELLATION_CODE                            |       1357919 |      17164879 |      3.11847e+07 |\n",
    "| CANCELLED                                    |             0 |             0 |      0           |\n",
    "| CARRIER_DELAY                                |       1115291 |      14159383 |      2.58899e+07 |\n",
    "| CDSD                                         |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| CLDD                                         |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| CRS_ARR_TIME                                 |             0 |             0 |      0           |\n",
    "| CRS_DEP_TIME                                 |             0 |             0 |      0           |\n",
    "| CRS_ELAPSED_TIME                             |             2 |            18 |    164           |\n",
    "| DATE                                         |             0 |             0 |      0           |\n",
    "| DAY_OF_MONTH                                 |             0 |             0 |      0           |\n",
    "| DAY_OF_WEEK                                  |             0 |             0 |      0           |\n",
    "| DEP_DEL15                                    |         42306 |        168666 | 475789           |\n",
    "| DEP_DELAY                                    |         42306 |        168666 | 475789           |\n",
    "| DEP_DELAY_GROUP                              |         42306 |        168666 | 475789           |\n",
    "| DEP_DELAY_NEW                                |         42306 |        168666 | 475789           |\n",
    "| DEP_TIME                                     |         42306 |        168666 | 470815           |\n",
    "| DEP_TIME_BLK                                 |             0 |             0 |      0           |\n",
    "| DEST                                         |             0 |             0 |      0           |\n",
    "| DEST_AIRPORT_ID                              |             0 |             0 |      0           |\n",
    "| DEST_AIRPORT_SEQ_ID                          |             0 |             0 |      0           |\n",
    "| DEST_CITY_MARKET_ID                          |             0 |             0 |      0           |\n",
    "| DEST_CITY_NAME                               |             0 |             0 |      0           |\n",
    "| DEST_STATE_ABR                               |             0 |             0 |      0           |\n",
    "| DEST_STATE_FIPS                              |             0 |             0 |      0           |\n",
    "| DEST_STATE_NM                                |             0 |             0 |      0           |\n",
    "| DEST_WAC                                     |             0 |             0 |      0           |\n",
    "| DISTANCE                                     |             0 |             0 |      0           |\n",
    "| DISTANCE_GROUP                               |             0 |             0 |      0           |\n",
    "| DIVERTED                                     |             0 |             0 |      0           |\n",
    "| DSNW                                         |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| DailyAverageDewPointTemperature              |       1399302 |      17314977 |      3.16243e+07 |\n",
    "| DailyAverageDryBulbTemperature               |       1398097 |      17308543 |      3.16157e+07 |\n",
    "| DailyAverageRelativeHumidity                 |       1399302 |      17314968 |      3.16243e+07 |\n",
    "| DailyAverageSeaLevelPressure                 |       1399305 |      17314992 |      3.16244e+07 |\n",
    "| DailyAverageStationPressure                  |       1398096 |      17308585 |      3.16159e+07 |\n",
    "| DailyAverageWetBulbTemperature               |       1399302 |      17314977 |      3.16243e+07 |\n",
    "| DailyAverageWindSpeed                        |       1398096 |      17308540 |      3.16157e+07 |\n",
    "| DailyCoolingDegreeDays                       |       1398097 |      17308543 |      3.16157e+07 |\n",
    "| DailyDepartureFromNormalAverageTemperature   |       1398110 |      17309484 |      3.16188e+07 |\n",
    "| DailyHeatingDegreeDays                       |       1398097 |      17308543 |      3.16157e+07 |\n",
    "| DailyMaximumDryBulbTemperature               |       1398096 |      17308540 |      3.16157e+07 |\n",
    "| DailyMinimumDryBulbTemperature               |       1398097 |      17308543 |      3.16157e+07 |\n",
    "| DailyPeakWindDirection                       |       1398108 |      17308878 |      3.1616e+07  |\n",
    "| DailyPeakWindSpeed                           |       1398107 |      17308800 |      3.16159e+07 |\n",
    "| DailyPrecipitation                           |       1398096 |      17308540 |      3.16156e+07 |\n",
    "| DailySnowDepth                               |       1399196 |      17322001 |      3.16381e+07 |\n",
    "| DailySnowfall                                |       1399196 |      17321518 |      3.16372e+07 |\n",
    "| DailySustainedWindDirection                  |       1398096 |      17308540 |      3.16156e+07 |\n",
    "| DailySustainedWindSpeed                      |       1398096 |      17308540 |      3.16156e+07 |\n",
    "| DailyWeather                                 |       1399836 |      17328415 |      3.16468e+07 |\n",
    "| ELEVATION                                    |             0 |             0 |      0           |\n",
    "| FIRST_DEP_TIME                               |       1392390 |      17234624 |      3.14703e+07 |\n",
    "| FLIGHTS                                      |             0 |             0 |      0           |\n",
    "| FL_DATE                                      |             0 |             0 |      0           |\n",
    "| HDSD                                         |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| HTDD                                         |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| HourlyAltimeterSetting                       |         63216 |        737197 |      1.33026e+06 |\n",
    "| HourlyDewPointTemperature                    |          4573 |         46933 |  81553           |\n",
    "| HourlyDryBulbTemperature                     |          4457 |         44816 |  75091           |\n",
    "| HourlyPrecipitation                          |        159030 |       1903094 |      3.62134e+06 |\n",
    "| HourlyPresentWeatherType                     |       1192768 |      15502810 |      2.8321e+07  |\n",
    "| HourlyPressureChange                         |        929961 |      11382209 |      2.08149e+07 |\n",
    "| HourlyPressureTendency                       |        929961 |      11382209 |      2.08149e+07 |\n",
    "| HourlyRelativeHumidity                       |          4721 |         47696 |  82467           |\n",
    "| HourlySeaLevelPressure                       |        153459 |       1689485 |      3.04765e+06 |\n",
    "| HourlySkyConditions                          |         32883 |        346042 | 733369           |\n",
    "| HourlyStationPressure                        |          9624 |        114437 | 321409           |\n",
    "| HourlyVisibility                             |          4087 |         42438 |  71263           |\n",
    "| HourlyWetBulbTemperature                     |         10258 |        120500 | 334311           |\n",
    "| HourlyWindDirection                          |          6071 |         74934 | 136469           |\n",
    "| HourlyWindGustSpeed                          |       1219342 |      15145887 |      2.75156e+07 |\n",
    "| HourlyWindSpeed                              |          4646 |         50907 |  89729           |\n",
    "| LATE_AIRCRAFT_DELAY                          |       1115291 |      14159383 |      2.58899e+07 |\n",
    "| LATITUDE                                     |             0 |             0 |      0           |\n",
    "| LONGEST_ADD_GTIME                            |       1392390 |      17234624 |      3.14703e+07 |\n",
    "| LONGITUDE                                    |             0 |             0 |      0           |\n",
    "| MONTH                                        |             0 |             0 |      0           |\n",
    "| MonthlyAverageRH                             |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDaysWithGT001Precip                   |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDaysWithGT010Precip                   |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDaysWithGT32Temp                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDaysWithGT90Temp                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDaysWithLT0Temp                       |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDaysWithLT32Temp                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDepartureFromNormalAverageTemperature |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDepartureFromNormalCoolingDegreeDays  |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDepartureFromNormalHeatingDegreeDays  |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDepartureFromNormalMaximumTemperature |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDepartureFromNormalMinimumTemperature |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDepartureFromNormalPrecipitation      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyDewpointTemperature                   |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyGreatestPrecip                        |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyGreatestPrecipDate                    |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyGreatestSnowDepth                     |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyGreatestSnowDepthDate                 |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyGreatestSnowfall                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyGreatestSnowfallDate                  |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMaxSeaLevelPressureValue              |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMaxSeaLevelPressureValueDate          |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMaxSeaLevelPressureValueTime          |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMaximumTemperature                    |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMeanTemperature                       |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMinSeaLevelPressureValue              |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMinSeaLevelPressureValueDate          |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMinSeaLevelPressureValueTime          |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyMinimumTemperature                    |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlySeaLevelPressure                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyStationPressure                       |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyTotalLiquidPrecipitation              |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyTotalSnowfall                         |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| MonthlyWetBulb                               |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| NAME                                         |             0 |             0 |      0           |\n",
    "| NAS_DELAY                                    |       1115291 |      14159383 |      2.58899e+07 |\n",
    "| NormalsCoolingDegreeDay                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| NormalsHeatingDegreeDay                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| OP_CARRIER                                   |             0 |             0 |      0           |\n",
    "| OP_CARRIER_AIRLINE_ID                        |             0 |             0 |      0           |\n",
    "| OP_CARRIER_FL_NUM                            |             0 |             0 |      0           |\n",
    "| OP_UNIQUE_CARRIER                            |             0 |             0 |      0           |\n",
    "| ORIGIN                                       |             0 |             0 |      0           |\n",
    "| ORIGIN_AIRPORT_ID                            |             0 |             0 |      0           |\n",
    "| ORIGIN_AIRPORT_SEQ_ID                        |             0 |             0 |      0           |\n",
    "| ORIGIN_CITY_MARKET_ID                        |             0 |             0 |      0           |\n",
    "| ORIGIN_CITY_NAME                             |             0 |             0 |      0           |\n",
    "| ORIGIN_STATE_ABR                             |             0 |             0 |      0           |\n",
    "| ORIGIN_STATE_FIPS                            |             0 |             0 |      0           |\n",
    "| ORIGIN_STATE_NM                              |             0 |             0 |      0           |\n",
    "| ORIGIN_WAC                                   |             0 |             0 |      0           |\n",
    "| QUARTER                                      |             0 |             0 |      0           |\n",
    "| REM                                          |           564 |          3262 |   7515           |\n",
    "| REPORT_TYPE                                  |             0 |             0 |      0           |\n",
    "| SECURITY_DELAY                               |       1115291 |      14159383 |      2.58899e+07 |\n",
    "| SOURCE                                       |             0 |             0 |      0           |\n",
    "| STATION                                      |             0 |             0 |      0           |\n",
    "| ShortDurationEndDate005                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate010                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate015                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate020                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate030                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate045                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate060                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate080                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate100                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate120                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate150                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationEndDate180                      |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue005           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue010           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue015           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue020           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue030           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue045           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue060           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue080           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue100           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue120           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue150           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| ShortDurationPrecipitationValue180           |       1401369 |      17344640 |      3.16731e+07 |\n",
    "| Sunrise                                      |       1398105 |      17308441 |      3.16152e+07 |\n",
    "| Sunset                                       |       1398104 |      17308438 |      3.16152e+07 |\n",
    "| TAIL_NUM                                     |          8187 |         26250 |  72188           |\n",
    "| TAXI_IN                                      |         44317 |        187549 | 500371           |\n",
    "| TAXI_OUT                                     |         43120 |        177274 | 484892           |\n",
    "| TOTAL_ADD_GTIME                              |       1392390 |      17234624 |      3.14703e+07 |\n",
    "| WEATHER_DELAY                                |       1115291 |      14159383 |      2.58899e+07 |\n",
    "| WHEELS_OFF                                   |         43120 |        177274 | 484887           |\n",
    "| WHEELS_ON                                    |         44317 |        187549 | 500371           |\n",
    "| WindEquipmentChangeDate                      |        489426 |       5997761 |      9.02695e+06 |\n",
    "| YEAR                                         |             0 |             0 |      0           |\n",
    "| dest_airport_lat                             |             0 |             0 |      0           |\n",
    "| dest_airport_lon                             |             0 |             0 |      0           |\n",
    "| dest_airport_name                            |             0 |             0 |      0           |\n",
    "| dest_iata_code                               |             0 |             0 |      0           |\n",
    "| dest_icao                                    |             0 |             0 |      0           |\n",
    "| dest_region                                  |             0 |             0 |      0           |\n",
    "| dest_station_dis                             |             0 |             0 |      0           |\n",
    "| dest_station_id                              |             0 |             0 |      0           |\n",
    "| dest_station_lat                             |             0 |             0 |      0           |\n",
    "| dest_station_lon                             |             0 |             0 |      0           |\n",
    "| dest_station_name                            |             0 |             0 |      0           |\n",
    "| dest_type                                    |             0 |             0 |      0           |\n",
    "| four_hours_prior_depart_UTC                  |             0 |             0 |      0           |\n",
    "| origin_airport_lat                           |             0 |             0 |      0           |\n",
    "| origin_airport_lon                           |             0 |             0 |      0           |\n",
    "| origin_airport_name                          |             0 |             0 |      0           |\n",
    "| origin_iata_code                             |             0 |             0 |      0           |\n",
    "| origin_icao                                  |             0 |             0 |      0           |\n",
    "| origin_region                                |             0 |             0 |      0           |\n",
    "| origin_station_dis                           |             0 |             0 |      0           |\n",
    "| origin_station_id                            |             0 |             0 |      0           |\n",
    "| origin_station_lat                           |             0 |             0 |      0           |\n",
    "| origin_station_lon                           |             0 |             0 |      0           |\n",
    "| origin_station_name                          |             0 |             0 |      0           |\n",
    "| origin_type                                  |             0 |             0 |      0           |\n",
    "| sched_depart_date_time_UTC                   |             0 |             0 |      0           |\n",
    "| two_hours_prior_depart_UTC                   |             0 |             0 |      0           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db2971b-083e-4039-ac75-723150436971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before data pre-processing, most of the nulls in the data appear on the weather-side; which for the most part seems related to either lack of weather status, were not reported at all, or faulty/troublesome machinery at the time of reporting. Nearly all nulls associated within flight specific information are related to flight being cancelled, thereby not being able to produce specific time based metrics such as taxi times, departure and arrival times, air travel time, etc. Subanalysis was perfomed on airport level, if the appearance of cancelled flight per date makes a substantial impact on the flight delays per those airports and the results showed that average/median delays at airport level remained relatively consistent regardless of a cancellation; however, further processes need to be defined to understand if a cancellation impacts subsequent use of the `TAIL_NUM` aircraft throughout the remainder of its given travel \"leg\". Additionally, subanalysis was performed in the types of cancellations between assigned `TAIL_NUM` and unassigned `TAIL_NUM` flights to see if perhaps one type of cancellation provides broader implication of delays per date at the airport level; although, airport flight with assigned `TAIL_NUM` produced more extreme delays the overall median and dispersion of delays at the airport level remained relatively consistent, as highlighted below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d29858d4-578f-4bb4-885d-dbac537abf5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/non%20null%20plot.png' style=\"width:75%;\">\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/null%20plot.png' style=\"width:75%;\">\n",
    "\n",
    "*Plots specifically showcases a 3 month timespan, however results appeared very similar for 1 and 5 year data*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2026dec6-bbeb-49a5-99f3-36914577d20f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Average Delay by Dataset\n",
    "\n",
    "Exploring the differences and similarities across the datasets is crucial to understand the variation in outcomes when looking at model results. Here we view the average ARR_DELAY (arrival delay) and DEP_DELAY(departure delay) across the 3 different OTPW and flights datasets. Departure delays, shown in blue, are consistently higher than arrival delays in all datasets. This suggests that flights could make up some time in the air, decreasing the average arrival delay. The 1 year flight dataset has the highest average departure delay around 11 minutes, while both the 3 month datasets have the highest arrival delay around 6 minutes. In general, the datasets have very similar delays across the same time frames. We see some differences likely due to the joins used to create the OTPW dataset, but overall, this shows good consistency that we hoped for. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/avgDelayDFS.png' style=\"width:75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f90eac-eec4-4a2d-9dc8-d69d415c0d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### KDE Of Delay Times Across OTPW and Flight DFs\n",
    "\n",
    "The following kernel density (KDE) plots explore the distribution of ARR_DELAY and DEP_DELAY across the different time frames we have in the OTPW and flights datasets. At a high level, we can see most flights had a minimal delay with their peaks centered around 0. Departure delays are more common than arrival delays (shown in orange), and the majority of severe delays (~1000 minutes) also fall under departure delays. We can see the scarceness of these severe delays shown by the outstretched right tail of the plots, colored in orange. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/kdeOTP3.png' style=\"width:150%;\">\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/KDEflights3.png' style=\"width:150%;\">\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/KDEOTP1year.png' style=\"width:150%;\">\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/KDEflights1year.png' style=\"width:150%;\">\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/KDEOTP5year.png' style=\"width:150%;\">\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/KDEflights5year.png' style=\"width:150%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c59d74-38a3-4ab0-80bd-c5c40e966708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Departure Delay Categorized Across Datasets\n",
    "The following buckets/categories were created to classify the departure delays into digestable bins:\n",
    "\n",
    "Early: < -15 mins\n",
    "\n",
    "On Time: < 15 mins\n",
    "\n",
    "Short delay: <= 60 mins\n",
    "\n",
    "Medium delay: <= 180 mins\n",
    "\n",
    "Long delay: >181 mins\n",
    "\n",
    "| DEP_DELAY_Bucket | flights 3m |flights 3m %     | flights 1y | flights 1y %     | flights 5y | flights 5y %     | otpw 3m  | otpw 3m %     | otpw 1y  | otpy 1y %     | otpw 5y  | otpy 5y %     |\n",
    "|------------------|------------|-------|------------|-------|------------|-------|----------|-------|----------|-------|----------|-------|\n",
    "| On Time          | 2,176,678  | 80.0% | 11,863,244 | 83.1% | 60,062,502 | 84.1% | 1,086,867| 78.4% | 9,377,678| 84.0% |25,558,960| 84.5% |\n",
    "| Short Delay      |   371,152  | 13.6% |  1,671,600 | 11.7% |  8,137,265 | 11.4% |   185,201| 13.4% | 1,386,378| 12.4% | 3,599,460| 11.9% |\n",
    "| Medium Delay     |   141,494  |  5.2% |    782,040 |  5.5% |  3,501,882 |  4.9% |    70,618|  5.1% |   551,168|  4.9% | 1,559,418|  5.2% |\n",
    "| Long Delay       |   107,380  |  3.9% |    442,762 |  3.1% |  2,065,284 |  2.9% |     49,230|  3.6% |   268,188|  2.4% |   797,588|  2.6% |\n",
    "| Early            |    10,238  |  0.4% |     84,428 |  0.6% |    410,500 |  0.6% |      5,066|  0.4% |    40,296|  0.4% |   157,693|  0.5% |\n",
    "| **Total**        | **2,806,942** |       | **14,843,074** |       | **71,177,433** |       | **1,387,982**|       | **11,623,708**|       | **30,673,119**|       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed1ccaf4-0014-4ca4-b893-3e7e556a7cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cancellation/ Diversion Rate Across Datasets\n",
    "| Dataset        | Cancelled (%) | Diverted (%) |\n",
    "|----------------|----------------|---------------|\n",
    "| flights 3m     | 3.10%          | 0.22%         |\n",
    "| flights 1y     | 1.82%          | 0.25%         |\n",
    "| flights full   | 1.84%          | 0.24%         |\n",
    "| otpw 3m        | 1.13%          | 0.08%         |\n",
    "| otpw 1y        | 1.54%          | 0.26%         |\n",
    "| otpw full      | 1.54%          | 0.25%         |\n",
    "\n",
    "This table is a great measure to show consistency across the datasets in terms of cancelllations and diversions. This suggests that we can expect to see no major outliers or biases when considering these 2 as factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b93d384-8bbd-43f6-8edf-0922c78f762c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Flight Count Breakdown Across Carriers\n",
    "\n",
    "Across the U.S the context for what an airline carrier signifies is truthfully, an extremely complex and nuanced conversation about reporting guidelines, carrier definitions, and to a certain extent open to interpretation. \n",
    "\n",
    "| OP_UNIQUE_CARRIER | CARRIER_NAME (2015â2019 context)                                                                 | 2015 | 2016 | 2017 | 2018 | 2019 |\n",
    "|-------------------|--------------------------------------------------------------------------------------------------|-------|-------|-------|--------|--------|\n",
    "| 9E                | Endeavor Air (Delta Connection regional carrier)                                                 | 0     | 0     | 0     | 245908 | 256647 |\n",
    "| AA                | American Airlines (merged with US Airways; full integration around 2016)                         | 725593 | 914254 | 893359 | 916626 | 944204 |\n",
    "| AS                | Alaska Airlines (acquired Virgin America in 2016; merged ops completed by 2018)                  | 172496 | 177260 | 184455 | 245710 | 264064 |\n",
    "| B6                | JetBlue Airways (independent low-cost carrier)                                                   | 262627 | 277888 | 293307 | 300678 | 291100 |\n",
    "| DL                | Delta Air Lines (legacy carrier)                                                                 | 875501 | 922264 | 921290 | 949127 | 990076 |\n",
    "| EV                | ExpressJet Airlines (United Express; Delta Connection until 2018)                                | 571870 | 490908 | 338926 | 202864 | 134408 |\n",
    "| F9                | Frontier Airlines (ultra low-cost carrier)                                                       | 90830  | 95115  | 102302 | 119094 | 134315 |\n",
    "| G4                | Allegiant Air (independent ultra low-cost carrier)                                               | 0      | 0      | 0      | 96109  | 104852 |\n",
    "| HA                | Hawaiian Airlines (independent full-service carrier)                                             | 76261  | 76775  | 79911  | 83680  | 83654  |\n",
    "| MQ                | Envoy Air (wholly owned by AA; rebranded from American Eagle in 2014, operated as AA Eagle)      | 294599 | 0      | 0      | 295978 | 326168 |\n",
    "| NK                | Spirit Airlines (ultra low-cost carrier)                                                         | 117233 | 137894 | 155864 | 175760 | 202980 |\n",
    "| OH                | PSA Airlines (owned by AA, operates as American Eagle)                                           | 0      | 0      | 0      | 278358 | 288626 |\n",
    "| OO                | SkyWest Airlines (regional carrier for DL, UA, AA)                                               | 588310 | 605823 | 703903 | 773498 | 832664 |\n",
    "| UA                | United Airlines (legacy carrier)                                                                 | 514806 | 543954 | 582005 | 620119 | 622441 |\n",
    "| US                | US Airways (merged with AA in 2013, brand retired in Oct 2015)                                   | 198710 | 0      | 0      | 0      | 0      |\n",
    "| VX                | Virgin America (acquired by Alaska in 2016, brand retired in 2018)                               | 61903  | 69121  | 70771  | 17669  | 0      |\n",
    "| WN                | Southwest Airlines (major low-cost carrier)                                                      | 1261545 | 1299021 | 1325372 | 1351975 | 1360300 |\n",
    "| YV                | Mesa Airlines (regional carrier for UA and AA)                                                   | 0      | 0      | 0      | 215125 | 227323 |\n",
    "| YX                | Republic Airways (regional carrier for UA and AA; formerly DL too)                               | 0      | 0      | 0      | 316082 | 328471 |\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/flight_by_carrier_yearly.png' style=\"width:75%;\">\n",
    "\n",
    "Here are a few bulletpoints/context for why suddenly some carrier reporting vanish for 2016 and 2017\n",
    "\n",
    "- **2015 - 2017**:  \n",
    "  - BTS required carrier reporting from carriers **with â¥ 1.0% of total domestic revenue** (based on trailing 12-months).\n",
    "  - Many regional carriers owned or controlled by subsidiaries, did not meet this theshold in 2016 and 2017\n",
    "  - Like consolidated by the mainline carriers AA, Delta, United\n",
    "  - Flights, not actually zero\n",
    "  - in 2015, either by chance, some carrier even regional met the threshold, or reporting may have become complicated due to AA merger with US Airlines\n",
    "\n",
    "- **2018**:\n",
    "  - DOTâs **Enhancing Airline Passenger Protections III** started\n",
    "  - Instead of being 1% revenue theshold, it was lowered to 0.5%\n",
    "  - Resulted in additional reporting\n",
    "\n",
    "\n",
    "- American Eagle is a subsidiary of AA, however, operates as it's own unique entity, but formally owned by AA\n",
    "- 9E (Endeavor Air, a Delta Affiliate)\n",
    "  - Flights reporting may have been absorbed under their main carrier DL, or under another regional affiliate, reporting guideline changes?\n",
    "- MQ (Envoy Air, American Eagle, AA Affiliate)\n",
    "  - Flights reporting may have been absorbed under their main carrier AA, or under another regional affiliate, merger or reporting guideline changes?\n",
    "- OH (PSA Airline)\n",
    "  - Flights reporting may have been absorbed under thier main carrier AA, or under another regional affiliate, merger or reporting guideline changes?\n",
    "- US (US Airways)\n",
    "  - Official retiring upon merger with AA, ceased all operations by Oct 2015\n",
    "- YV (Mesa Airline)\n",
    "  - serves as subcontract for United and AA, likely absorbed by these airline reporting in this span, or possibly due to AA merger, reporting guideline changes?\n",
    "- YX (Republic Airlines)\n",
    "  - Filed 2016 bankruptcy, possesses combined contracts between AA, United, and Delta; likely pre-2018 their number absorbed by these carriers\n",
    "\n",
    "Therefore, we must further explore, truly how useful `OP_UNIQUE_ID` can be or if perhaps, we are able to extract our own carrier identifier from the available information, perhaps noted by the tracking of `TAIL_NUM` and seeing if in years 2016 and 2017, that aircraft appears assigned under the larger conglomerate of AA, United, or Delta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "551f0c57-0a25-46d4-ad9f-28978cad0328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unique Aircrafts Over the Years\n",
    "\n",
    "| Year        | Unique Aircraft Count |\n",
    "|----------------|----------------|\n",
    "| 2015    | 4897          | \n",
    "| 2016     | 5033         | \n",
    "| 2017   | 4791          | \n",
    "| 2018        | 5718          | \n",
    "| 2019        | 5884          | \n",
    "\n",
    "The number of unique aircraft appears to increase slightly from 2015 - 2019, with a noticeable jump from 2017 to 2018. The dip in 2017 is something to note, with potentially fewer planes in operation that year due to maintenance, forecasted travel plans, or potentially being unreported as show in carrier breakdown earlier due to pre-2018 requiring carrier reporting only in carriers that account of >1% of domestic revenue. This supports the idea that travel at least at surface level, appears to be trending up over the years and at the very least, is relatively consistent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcd81f77-4b6b-46a4-a934-56a14bb863c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Aircraft Use\n",
    "\n",
    "In terms of yearly aircraft use, most aircraft were used more than 260 days out 365 days a year. To be more precise, for the year 2015, 1,118 aircraft were used less than 260 days of the year, that accounts for only 22.8% of all aircraft. As a frame of reference, 260 days is approximately all weekdays in a calendar year. However, this result is a slightly misleading, as not all aircraft start on the same day, new aircraft could be introduced through the year or even older aircraft that experienced severe maintenance work could be introduced into the airline fleet. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/days_used_yr.png' style=\"width:75%;\">\n",
    "\n",
    "*For simplicity, plot above highlights histogram of active days of use for aircrafts in 2015*\n",
    "\n",
    "However, since we do not have information whether an aircraft is newly introduced to the fleet or has been under maintenance for an extended period (without utilizing some extraneous datasets), we instead will take into account the first date that an aircraft has appeared within our dataset, as shown with the distributions below, we can easily note that the vast majority of aircraft appear to be utilized throughout the entire year, as shown by the two plots below which show the distribution of aircraft counts based on their first appearance and last appearance in the dataset. For first date appearance, only 498 aircraft appear outside of the first month of 2015, that is only about 10.2% of all aircraft. For last date appearance, only 482 aircraft appear outside of the last month of 2015, that is only about 9.8% of all aircraft. \n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/first_date_appearance.png' style=\"width:75%;\">\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/last_date_appearance.png' style=\"width:75%;\">\n",
    "*For simplicity, plots above highlights histogram of active days of use for aircraft in 2015*\n",
    "\n",
    "\n",
    "All of this is particularly noteworthy when we visualize the averaged departure delays across all aircraft and can note what appears to be a negative trend where delays appear to lower overall as an aircraft experiences more days of activity. This has therefore allowed for the development of a new engineered feature, discussed further below under **Feature Engineering** called Active Flight Score (*AFS*), that models the percent usage of an aircraft utilizing windowed statistics from all data prior to current date. \n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/dep_delay_active.png' style=\"width:75%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d22e94f-b06a-41e6-bbb1-da323f93180d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carrier Changes\n",
    "\n",
    "Throughout our various datasets, there exist instances where are an aircraft changes their assigned carrier, we do not have information to understand the exact context behind the change but theorize it can a purchase by another carrier, or possible aircraft loans by carriers. However, in comparing average delays between aircrafts that had no change, changed to one different carrier, or changed to two different carriers, we did not find notable differences in the average delay. Just given the sheer volume of Single Carrier vs Double/Triple Carrier, we do not consider this to do a major contributory factor, especially when viewing the Q1, Q3, and median values across the 5 year data as shown below. \n",
    "\n",
    "| Carrier Changes | Count (3mo) | Count (1yr) | Count (5yr) |\n",
    "|----------------|----------------|----------------|----------------| \n",
    "| Single Carrier    | 4561          | 4560 | 7353 |\n",
    "| Two Carriers     | 0         | 337 | 539 |\n",
    "| Three Carriers   | 0          | 0 | 2 |\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/carrier_change_delay.png' style=\"width:75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27158f63-0d1a-4d20-8bd3-6f036fe9cf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Holidays\n",
    "\n",
    "We briefly explored how the much the average delay changed between holiday dates vs non holiday averages per airports. The plot below was constructed utilizing window of +/- 7 days of Federal holidays per **holidays** python library. However, we did note a few inconsistencies, the extended one week was particularly effective for holidays in Q4 and Q1 which capture Thanksgiving, Christmas, and New Years where were denote with the postive Î between holiday departure averages and non-holiday departure averages, however for Q2 and Q4, it was effectively neutral or even negative. The suspicion is captured addtional extended the window too much for non-impact holidays like President's Day or Memorial Days that may actually be reducing the average holiday delays since people are not likely to fly a week before or after. Also, we are potentially missing key dates like Summer Vacation, Spring Break, college student return to home or school, etc. Therefore, for the further pipelines and time being, we are limiting to within three days in our pipeline until we can better redefine \"holiday travel\" in a meaningful way. Also of note, days surrounding holidays tend to see a greater increase in departure delays than the holiday itself. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/holidays.png' style=\"width:75%;\">\n",
    "\n",
    "To better guide our understanding, we have looked at the overall distribution of delays, and would likely include a combination of shorter and long window delays as noted in the plot below for average departure delay per each date. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/acc3f9a400180ecc66acc52dd73fef104c336f71/Media/delay_yr.png' style=\"width:75%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c79ed7c0-2b21-4e3f-b70d-276ebf9db3a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Feature Engineering\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd79056d-b4a2-4c66-9c29-88a578599aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Relevance of Weather: Seasonal and Geographic Trends\n",
    "\n",
    "In the opening phases of this project, our group strongly believed that weather would be a key factor in delay prediction. In fact, one of our group members experienced a weather delay in our first week of working together. Through some additional research, we uncovered a report from the Federal Aviation Administration (FAA), which cited that 74.26% of flight delays between June of 2017 and May of 2023 were caused by weather conditions.\n",
    "\n",
    "In our initial analysis of our own datasets, we uncovered that relatively few delayed flights cited weather as the primary reason for delay. Over the entire on-time performance dataset, running from 2015 to 2019, we found that only 1-2% of flights were weather delayed. That being said, we believe that the ripple effects possible from weather delays could have significant effects on subsequent flights, and we elected to proceed with our attempts to use weather information in delay prediction.\n",
    "\n",
    "Before we could do so, we ran additional EDA on the weather variables available to us in our primary dataset. We began by taking a look at the distribution of weather variables. Our data included information about air temperature, dew point, precipitation, wind speed, visibility, and humidity. Below is a look at the distribution of each of these variables.\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/WeatherVariables.png' style=\"width:75%;\">\n",
    "\n",
    "All of these variables appear to be skewed in some sense, and some, like precipitation, wind speed, and visibility, have outliers in the dataset. As these outliers appear to be a result of errors in logging the data, we will drop those above the 99th percentile in our analysis. \n",
    "\n",
    "While the overall number of delays due to weather was low, we speculated that there may be seasonal or geographic trends that affect the percentage of weather delays in the dataset. Consistent with our expectations, we saw trends in both seasonality and region. \n",
    "\n",
    "First, we examined geographic regions with the largest percentages of flights delayed due to weather.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/WeatherDelaysByRegion.png' style=\"width:75%;\">\n",
    "\n",
    "States like Illinois, South Dakota, and Georgia saw the highest percentage of weather delays, followed closely by Iowa, West Virginia, Florida, and Texas. Each of these regions tends to experience some extreme weather, whether it be freezing temperatures and snow in areas like Illinois or South Dakota, or tropical storms in areas like Georgia and Florida. Considering the differences in the types of weather events that these regions experience, let's take a look at seasonal trends to determine if certain types of weather events are more likely to cause delays.\n",
    "\n",
    "| Season                   | Total Flights | Weather Delays | % Weather Delay |\n",
    "| :----------------------: | :-----------: | :------------: | :-------------: |\n",
    "| Spring (Months 3-5)      | 8045540       | 73789          | 0.9171          |\n",
    "| Summer (Months 6-8)      | 8395225       | 118405         | 1.4104          |\n",
    "| Fall (Months 9-11)       | 7812924       | 48589          | 0.6219          |\n",
    "| Winter (Months 12, 1, 2) | 7419430       | 90167          | 1.2153          |\n",
    "\n",
    "Here, we can see that there were a greater number of flights in the spring and summer months than the fall or winter months. Importantly, we can also see that flights during the summertime were the most likely to be delayed for weather conditions at 1.41%. The winter months followed as the second most likely with 1.22%, while the spring and fall saw less than 1% of flights delayed due to weather. \n",
    "\n",
    "These trends are supported by the following visualization which shows trends over time throughout the dataset. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/WeatherDelaysByMonth.png' style=\"width:75%;\">\n",
    "\n",
    "Taking a look at the percentages of weather-delayed flights across each month of the dataset, we see the greatest percentage of delays in the earlier and middle months of each year, while there are far lower percentages each fall. \n",
    "\n",
    "Finally, weâll take a look at the consistency of weather delays year to year. \n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/AnnualWeatherDelayRate.png' style=\"width:75%;\">\n",
    "\n",
    "In the above visualization, it is apparent that percentages of weather delays are not consistent year-to-year in our dataset. Upon further investigation, it would appear that a significantly lower percentage of flights may have been delayed due to weather conditions in 2016 because of the lack of extreme/severe weather events that took place in that year. The National Weather Service lists only four severe weather events in 2016: three of which were tornados and one was heavy rain. Conversely, 2017 saw several hurricanes and tropical storms. While this would suggest that there should be a higher percentage of weather-delays, that is not what we see in our graph, however, it is important to note that these severe weather events likely resulted in a greater number of cancellations, which would not appear as weather delays in our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58333e9f-f01e-446e-8cc6-d01722e3bab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating âAdverse Weatherâ Variable and Generating Predictions for Departure Time\n",
    "\n",
    "In order to make use of the weather data available and prevent leakage, we initially planned to predict whether there would be adverse weather two hours before each flightâs scheduled departure time. This would eventually enable us to feed our adverse weather prediction into our downstream delay prediction model without fear of our model peeking into the actual weather conditions at departure time. \n",
    "\n",
    "At this phase in our project, we elected to create a proof of concept model to determine how likely we were to be able to accurately predict adverse weather conditions based on the information available to us. This would be an isolated experiment, using slightly different strategies than our other engineered features, with the plan to eventually make modifications to this procedure and add this model into our larger feature engineering pipeline.\n",
    "\n",
    "For this experiment, we elected to use a single year of data, from both our joined dataset and our isolated weather dataset. We worked under the assumption that a single year would be enough data to encapsulate seasonal trends, but would allow us to work more quickly than working with the entire corpus of data available to us. \n",
    "\n",
    "We proceeded with the year 2019, and began by preparing our joined dataset to only include necessary columns to be used as keys for individual flights. We kept columns such as tail number, carrier, and origin airport, as well as the âtwo hours priorâ and scheduled departure timestamps. \n",
    "\n",
    "Next, we built our weather features. We began by identifying the roughly 300 stations in our dataset that contained flight information, and then inner-joined the full weather dataset to those stations, dropping the other 95% of records in the weather dataset immediately.\n",
    "\n",
    "We cast relevant fields to numeric and handled null values. In this case, we elected to use mode imputation for null observations at any station that reported weather data, working under the assumption that, for any station that had non-null values in its dataset, the null values represented the standard state (i.e., no precipitation, clear visibility, no wind, etc.). If a station contained no weather observations, this station was dropped from this dataset.\n",
    "\n",
    "At this point, we generated a binary âadverse_weatherâ variable at each hour. This variable serves as a simplistic indicator of whether or not a station was observing adverse weather at each hour. We generated this variable using a data-driven approach: for all flights in the dataset that were delayed due to weather conditions, we took the upper-percentiles of the weather states that were occurring. For example, we took the 90th percentile of precipitation and wind gusts from those observations, as well as the 75th percentile of wind speed. If an observation in the dataset cleared any of the thresholds we were using, we were said to have adverse weather.\n",
    "\n",
    "We also took a critical step to prevent leakage in this modelâs prediction: creating lagged variables. We generated lagged features 1-4 hours prior to the prediction (3-6 hours before scheduled departure). As an additional check, we dropped any flights from this dataset that lacked lagged variables after said variables were created. \n",
    "\n",
    "Now that our features were built for adverse weather prediction, we moved onto splitting the data. We parsed our scheduled departure timestamps into epoch seconds, and used this to find the 75th-percentile cutoff for our 2019 dataset. We split the data into 75% training data (before cutoff) and 25% testing data (after cutoff). Note that in this proof of concept, we may miss trends due to seasonality, but when we apply this methodology to the full dataset across multiple years, we expect more reliable results. \n",
    "\n",
    "At this point, we used VectorAssembler to bundle the 24 lag columns into a single âfeaturesâ vector, and used StandardScaler to normalize each feature to unit variance.\n",
    "\n",
    "Finally, we were onto model training. We elected to train a random forest classifier and a logistic regression model (which would serve as our baseline). For each model, we computed the area under the curve, accuracy, precision, recall, and F1 score. The results from our evaluations are below.\n",
    "\n",
    "| Evaluation Metric | Logistic Regression | Random Forest Classifier |\n",
    "| :---------------: | :-----------------: | :----------------------: |\n",
    "| Accuracy          | 0.7464              | **0.7562**               |\n",
    "| AUC               | 0.7893              | **0.8249**               |\n",
    "| F1                | 0.7444              | **0.7573**               |\n",
    "| Precision         | 0.7452              | **0.7618**               |\n",
    "| Recall            | 0.7465              | **0.7562**               |\n",
    "\n",
    "In order to determine which variables in our dataset were most relevant to our predictions from the random forest model, we ran a test of feature importance. We used Scikit-learnâs built-in feature importances function, and uncovered that visibility and wind-speed in the hours leading up to our prediction were the most relevant in determining whether or not adverse weather would be present at a scheduled departure time. \n",
    "\n",
    "This experiment gave us confidence in our predicted adverse weather variableâs ability to help generate more accurate predictions in our overall model, and we proceed to further edit and fine-tune the weather prediction model in the final phase of our project. \n",
    "\n",
    "We began by tuning the adverse weather variable thresholds in order to achieve a closer match to the percentage of flights that were delayed due to weather conditions. Through this process, we were able to identify more extreme weather conditions that could be more likely to predict a flight delay. \n",
    "\n",
    "Next, we switched to MinMax scaling and proceeded with the development of an XGBoost model. We were able to achieve satisfactory results, and elected to incorporate our predictions into our feature engineering pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a25277f1-0fee-4772-a2b5-a0f93fa6246e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Change of Plans: Replacing Weather Prediction with Lagged Variables and Our âAdverse Weatherâ Flag\n",
    "\n",
    "At this point, we immediately realized that incorporating the predicted variables into our pipeline was far more complex than we had anticipated. Questions arose around when the predictions would be made, how we would handle predictions for weather versus predictions in the overall delay-prediction models, and whether there might be a more straightforward solution that could yield results that were as strong, or even better, than the proposed multi-stage process.\n",
    "\n",
    "We discussed the issue, and ultimately elected to move away from the predicted âadverse weatherâ solution entirely. We did, however, think that we had uncovered a way to incorporate weather into our feature engineering process during the development of the initial weather prediction models: lagged variables and an âadverse weatherâ flag.\n",
    "\n",
    "We proceeded with a slightly altered approach to generating these variables. We used the supplementary weather dataset made available to us by NOAA, and pulled observations of weather variables between two and six hours before each flightâs scheduled departure time. Taking the first observation at each âstation hourâ during this time range, we were able to trace weather patterns in the hours leading up to prediction (two hours ahead of scheduled departure).\n",
    "\n",
    "We applied similar preprocessing as before:\n",
    "- For simplicity, we only looked at stations with associated flight data in our primary dataset instead of attempting to create a more complete mapping from nearby stations.\n",
    "- We used mode imputation to address scenarios in which âstandardâ weather conditions were included as null values at stations that regularly reported weather data. This means that, for any included station with non-null values in the weather-related fields, null values were replaced with zeros for fields such as precipitation and wind, or with ten for fields like visibility. \n",
    "- Finally, we elected to drop outliers that exceeded the 99.9th percentile in columns such as precipitation and wind speed, as these values appeared to be the result of recording errors (appeared higher than realistically possible, rainfall of hundreds of inches in an hour, or wind speeds of thousands of miles per hour).\n",
    "\n",
    "Once this pre-processing was complete and the lagged variables were created, this process was built into a self-contained function and incorporated into our feature engineering pipeline. \n",
    "\n",
    "In addition to the lagged variables, we created an âadverse weatherâ flag at the time of prediction. This binary variable was constructed using the tuned thresholds that we had found during our weather prediction development, but this time, the variable was based on conditions two hours prior to each flightâs scheduled departure time.\n",
    "\n",
    "This feature was added to our function to produce weather features, and incorporated into our feature engineering pipeline alongside the lagged variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65498d32-5207-4121-8218-c00fb5e488ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Flights, Planes, and Airports\n",
    "\n",
    "\n",
    "We primarily focused on opportunities to generate more sophisticated features that were based directly on flight and airport data. All features were time filtered to avoid the previously mentioned constraints around data leakage. They are as follows:\n",
    "\n",
    "\n",
    "- Flights were joined by their tail number to the most recent preceding flight and we then extracted the preceding flight's arrival delay, departure delay, and time from arrival to scheduled departure of current flight. Previous flight arrival delay would be an ideal piece of information to have but is usually not available, whereas previous departure delay is. Since they are so closely correlated (Pearson's r of ~0.96), we have opted to use the previous departure delay. This would correspond to a given flight not being able to compensate much for a delay in departure by speeding up in the air.\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/dep_arr_correlation.png' style=\"width:75%;\">\n",
    "\n",
    "\n",
    "- We also gathered delay and time gap values for the flight before the preceding one, with a similar rationale/method as described above. The motivation for this was to capture any longer-term relationships around propagating delays as opposed to simply examining delays one flight into the past.\n",
    "\n",
    "\n",
    "- We generated new columns around the scheduled departure time of the next flight as well as the time gap between that and the scheduled arrival of the current flight. The rationale was that airlines might be likely to deprioritize the current flight when there is a bottleneck and the next flight has a lot of slack built in (i.e. a planned delay would not be likely to cause downstream delays).\n",
    "\n",
    "\n",
    "- Scheduled departure times were converted from HHMM format to a minutes only format to generate a proper numeric variable.\n",
    "\n",
    "\n",
    "- Graphs were generated on a monthly basis, with airport codes represented by nodes and the reciprocal of flight count on a given route forming edge weights. This created a network that would have lower weights on higher volume routes. Shortest path algorithms would get routed through higher volume routes preferentially. This way when we calculated centrality measures, the airports that act as hubs would get labeled with higher centrality. The features for any given flight were derived from the previous slice in time so that there would be no inclusion of information that wouldn't be present at the time of prediction. The centrality metrics we will be experimenting with are Betweenness, Degree, and Closeness centrality. They will be encoded as features for both the origin and destination airport for each flight.\n",
    "\n",
    "\n",
    "- Interaction features were explored between high ranking features that are not correlated (simple products) but they were not found to increase model performance in any noticeable way. This is likely because the model architectures themselves often have the capacity to learn these interactions.\n",
    "\n",
    "\n",
    "- Airports visited by a given plane through the span of the dataset were also calculated . This would directly correspond to the diversity of routes a given plane has seen, and might imply something about changing staff, maintenance concerns, or logistical changes.\n",
    "\n",
    "\n",
    "- Neighboring airports of the origin are also being counted. The thought process here is that a nearby airport could cause crowding of airspace and contribute to delays. The current radius we're using for this feature is 100 miles.\n",
    "\n",
    "\n",
    "- Flights are evaluated to see if they fall within 3 days of an official holiday. The thinking here is that the holidays could cause increased flight volume and delays could propagate through the system at a greater rate with the reduced slack.\n",
    "\n",
    "\n",
    "- We are also modeling the relative age of each plane based on its first flight in the dataset. Using this, we are also calculating the rate of utilization in its lifespan within the scope of the dataset utilizing lagged statistics. Utilization could correspond to trends around maintenance concerns, which are often causes of delays when done on an impromptu basis.\n",
    "\n",
    "\n",
    "- We are looking at flagging flights that are scheduled to take off on the same day and time with the same tail number. This is because cancellations with a given plane are still reported, even when they are repurposed for a different destination. We are considering these to be \"redirected\", which could have implications around logistical readiness (eg a flight moved to a new route might need some time to confirm details for the new route).\n",
    "\n",
    "\n",
    "- Relative airport traffic is being modeled by taking the flight count at the origin airport on the previous day and then dividing by the average daily flight count of the airport for the previous calendar month. The thought here is that surges in airport traffic could strain logistics and cause systematic delays.\n",
    "\n",
    "\n",
    "- Finally, we are also taking counts of flights going to the same destination that were cancelled or diverted (separate counts/features for each) in the 2-8 hours before the current flight. This could create some signal around weather or other logistical patterns on routes that share proximity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1e9392a-1971-475a-afdb-9840404ff3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ce9f67-b30a-4c4d-b874-29357f368b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "The following scree plot explores the variance covered by the top 10 principal components, applied to df_flights of 1 year. PC1 dominates the other components, explaining ~16% of the variance, with PC2 and PC3 also contributing a good amount with ~12% each. There are diminishing returns after PC4 which is known as an 'elbow pattern' around PC3-PC5. The top 10 PCs shown here account for ~85% of the variance which is great dimensionality reduction from our original 22 feature input list (shown below). This shows great promise that most of the meaningful signal can be compressed into fewer dimensions than we have from our raw data. We can combine redundant or noisy features, and potentially replace original features with fewer principal components without sacrificing predictive power. In regard to our regression model, we can reduce overfitting by reducing dimensions, and stabilize our generalization by decreasing variance in our model. This is a great and crucial step toward enhancing our model performance. \n",
    "The input list for this PCA was created by taking numeric fields from df_flights that would avoid data leakage. The list is as follow: \n",
    "[`QUARTER`,`MONTH`,`DAY_OF_MONTH`,`DAY_OF_WEEK`,`OP_CARRIER_AIRLINE_ID`,`OP_CARRIER_FL_NUM`,`ORIGIN_AIRPORT_ID`,`ORIGIN_AIRPORT_SEQ_ID`,`ORIGIN_CITY_MARKET_ID`,`ORIGIN_STATE_FIPS`,`ORIGIN_WAC`,`DEST_AIRPORT_ID`,`DEST_AIRPORT_SEQ_ID`,`DEST_CITY_MARKET_ID`,`DEST_STATE_FIPS`,`DEST_WAC`,`CRS_DEP_TIME`,`CRS_ARR_TIME`,`CRS_ELAPSED_TIME`,`DISTANCE`,`DISTANCE_GROUP`,`YEAR`]\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/PCAk10.png\">\n",
    "\n",
    "This visual explores how many principal components would be needed to cover 100% of the variance, which looks to occur around PC17. That means 5 of these original dimensions are adding no value and we can explore this further to ensure we remove all waste from our model inputs. \n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/PCAexplainedK22.png\">\n",
    "\n",
    "We have a dataframe breaking down each PC based on it's field makeup. For example, PC1 is represented by flight length and duration (`DISTANCE`, `DISTANCE_GROUP`, `CRS_ELAPSED_TIME`), while PC2 is influenced by location identifiers (`ORIGIN_AIRPORT_ID`, `ORIGIN_AIRPORT_SEQ_ID`, `ORIGIN_CITY_MARKET_ID`). this provides great meaning to our PCA work and allows us to map these components to the flight fields we are all familiar with. The following graph visualizes this connection for PC1 and PC2.\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/PCAtopfeatureWeights.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436d6a7e-5314-4b73-a506-bf39656ead54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Modeling Pipelines\n",
    "\n",
    "We implemented multiple modeling pipelines covering different methodological families:   \n",
    "- **Baselines** â included naive constant predictors and simple linear models to establish performance floors.  \n",
    "- **XGBoost regression** â Tree-based, non-linear modeling with strong handling of mixed feature types.  \n",
    "- **Classification approaches** â applied when reframing delay prediction as a categorical outcome. \n",
    "- **Neural network approach** â tested deep learning architectures for capturing complex, non-linear patterns.  \n",
    "\n",
    "These pipelines share a common high-level process:  \n",
    "1. **Feature preprocessing** â imputation of missing numeric values, categorical encoding, and vectorization into a single feature column.  \n",
    "2. **Target transformation** â log1p transformation for skew reduction in regression tasks.  \n",
    "3. **Model training** â model-specific training logic and hyperparameter tuning.  \n",
    "4. **Evaluation** â time-aware validation strategies with metrics suited to each task type (e.g., RMSE for regression, precision/recall for classification).  \n",
    "\n",
    "This unified structure allows consistent comparison across modeling strategies while accommodating the unique requirements of each approach.\n",
    "\n",
    "### Model Pipeline Workflow \n",
    "\n",
    "<img src=\"/Workspace/Users/apl@berkeley.edu/Team_4_3/Media/pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ead02ce-b998-4b76-aff1-9df2f3abb749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://github.com/mesighel/mids_w261/blob/main/Media/pipeline.png?raw=true\" width=\"100%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b52f1014-6357-4f3e-a3c5-3140f275bbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Classification Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c84587df-3d60-44e2-a4d5-effc762421c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delay detection via classification was conducted as an initial filter of sorts, to help operations focus only on flights that were considered 'at risk'. While the regression modeling that follows can predict delay as a continuous variable, classification serves a broader purpose of simply alerting fliers and airline carriers whether a given flight will be delayed: yes or no. To build on that, we developed regression models to quantify the severity of these delays. We can combine the work from both approaches to provide a complete, robust solution and answer to flight delays. Imagine we take a single flight, from JFK to SFO, we can use the classifier to initially understand if we can expect a delay at all, and if there is, then we can invoke our regression model to quantify the delay. They work hand in hand but can also operate independently, showing the power of each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd49feac-89c8-4469-ba49-d0701613e6ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#### _All of the following models were trained and tested using a temporal split, where 2015-2018 served as the training data and 2019 served as the test set. This avoids data leakage and potential violation of any temporal dependencies. Unless noted otherwise, this is how training and testing were conducted._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "352fa4dd-4c12-40a3-a581-919ebc931dfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Baseline Classification Using Logistic Regression\n",
    "\n",
    "Before experimenting with more complex models, we elected to construct a standard logistic regression model that could serve as a baseline for delay classification. A reminder that a flight delay is not classified as an official delay until departure is more than 15 minutes behind schedule. \n",
    "\n",
    "We began with some standard preprocessing on our complete dataset, now including our complete set of engineered features. At this stage, we dropped any rows that still contained missing values, filtered out cancelled/diverted flights, and created a binary âlabel = 1â if the departure delay was greater than 15 minutes. We also cast numeric and binary columns from strings to doubles, and performed our train/test split in a way to account for leakage, assigning all years prior to 2019 as the training dataset, and 2019 as the test set. \n",
    "\n",
    "Next, the fields for tail number, origin, destination, and carrier were indexed and one-hot encoded. All encoded categorical, binary flags, graph features, and weather/time-lagged variables were assembled into a feature vector, and then scaled using a MinMax scaler.\n",
    "\n",
    "Fitting a baseline logistic regression model yielded an AUC of 0.6262 and an F1 score of 0.7119.\n",
    "\n",
    "For tuning, we kept the existing features and used logistic regression. We first handled class imbalance by applying inverse-frequency class weights (wââ1.25 for on-time, wââ5.07 for delayed) and trained a weighted model. We then ran a 3-fold cross-validation grid over the regularization settings and found the best parameters to be regParam = 0.001 and elasticNetParam = 1.0 (pure L1). Next, we swept decision thresholds from 0.30 to 0.70 and chose 0.60 because it gave the highest F1 on the 2019 test set. The L1 penalty produced a sparse model with only 21 non-zero coefficients (e.g., DEST_vec, DAY_OF_WEEK, DISTANCE), which suggests a compact feature set generalizes best.\n",
    "\n",
    "At this point, we achieved the following results for our 2019 test set:\n",
    "| Evaluation Metric  | Logistic Regression Results |\n",
    "| :---------------:  | :-------------------------: |\n",
    "| AUC                | 0.6504                      |\n",
    "| Accuracy           | 0.7377                      |\n",
    "| Weighted Precision | 0.7184                      |\n",
    "| Weighted Recall    | 0.7377                      |\n",
    "| F1 Score           | 0.7268                      |\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/LogisticRegressionIMGs.png' style=\"width:75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a86b5b8-717d-4d5b-b75a-b7ca8f3d550b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Random Forest Model\n",
    "\n",
    "We then developed a Random Forest Classifier to predict delay as a binary outcome, where delay = YES (1) when delay is 15 minutes or greater. Otherwise the class identified is NO (0) meaning no delay which everyone loves to see. The model uses 34 features capturing schedule patterns, operational history, airport characterstics, and environmental indicators. RF was chosen for it's ability to handle nonlinear and complex relationships, through trees exploring different feature splits, as well as it's robustness to outliers and missing data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86b870c8-d35b-480a-ab5b-16583f32da07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Results of Random Forest Classification \n",
    "\n",
    "Time to execute: 3 minutes\n",
    "\n",
    "| Metric | Value           |\n",
    "|-----------------------------|-----------------------|\n",
    "| ROC AUC                     | 0.6727   |\n",
    "| PR AUC                     | 0.3604       |\n",
    "| Accuracy                      | 0.592       |\n",
    "| F1-Score                     | 0.627           |\n",
    "| Precision                      | 0.3072           |\n",
    "| Recall                      | 0.6834          |\n",
    "| Confusion Matrix                      | TP = 10,469. FP = 23,611. FN = 4,849. TN = 30,820.          |\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://github.com/mesighel/mids_w261/blob/main/Media/RfClassifierROC.png?raw=true' >\n",
    "\n",
    "The receiver operating characteristic (ROC) curve displays the model's ability to distinguish between delayed and on-time flights (AUC = 0.673), above the 0.5 value representing random chance, but still below stellar classification performance. AUC is a great metric to assess the performance of this model based on its threshold independence, ability to capture TP vs FP trade-off, and unbiased performance on imbalanced classes which is major for a dataset of this scale. \n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/RFClassifierThresholdTuning.png?raw=true' >\n",
    "\n",
    "At the default threshold of 0.5, the model favos recall (68%) over precision (31%). Threshold tuning analysis shows that decreasing the threshold increases recall at the expense of precision, while higher thresholds yield greater precision but diminished recall. This tradeoff can be tuned based on business use-cases, for example, if the ultimate goal is to catch the most delays for mitigation planning we can choose to optimize maximum recall for the best results. \n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/RFClassifierCM.png?raw=true' >\n",
    "\n",
    "False negatives (FN) show that 4,489 flights (6.9%) that were delayed 15 minutes or more were missed. This represents missed opportunities for proactive delay mitigation. This number is relatively low which means our model is not at major risk of being biased in this regard.\n",
    "False positives (FP) show that 23,611 flights (33.85%) were predicted as delayed but were actually on time, potentially leading to major false alarms that could misconstrue a passenger or the airline carrier expecting a delay when in reality, there is none. Over a third of our results falling into this category is dangerous, and will require specific tuning to avoid further issues in this category. \n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/RFClassifierFeatureImportance.png?raw=true' >\n",
    "\n",
    "The engineered features clearly dominate in terms of influence, which was expected, and show to be a great help in improving overall model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f737a121-3655-4bf0-8c71-e70618f4c93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Next Steps for Random Forest Classification\n",
    "\n",
    "The random forest model performs above average at best, but shows great promise from initial experimentation. Precision is observed to be low at the default threshold of 0.5, meaning many on time flights are incorrectly classified as delayed. While this dataset is already massive, having more data to feed the model will only facilitate model classification. Threshold optimization will help us mitigate the cost benefit tradeoffs between missed delays and false alarms. Feature enrichment such as the addition of high resolution weather, live traffic load, and other artificially generated features can help guide our model. Finally, we have discussed the idea of using segmented models, which include training specialized models per airline carrier, route category, or location to capture local delay dynamics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff5f9ca3-7575-4ca2-8e66-d4179dc80826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CatBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c67b5f-9a2d-43ca-86be-51a41e4c9ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on our results from XGBoost we found Catboost to be a potential improvement on model selection based on its handling of categorical features. Instead of implementing one hot encoding, catboost automatically avoids potential issues such as massive feature dimensions, data leakage, or high requirement for memory. Catboost uses gradient boosting and builds models sequentially which corrects residuals at each step. Being a gradient boosting decision tree model, this can also model complex interactions without having to explicitly engineer interaction terms. Given the number of NULL values, catboost can also handle NaNs for both categorical and numerical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e2e4ba-7077-4910-9754-0ecf8cc81b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####Key Results of CatBoost Classification\n",
    "\n",
    "Time to execute: 3 minutes\n",
    "\n",
    "| Metric | Value           |\n",
    "|-----------------------------|-----------------------|\n",
    "| ROC AUC                     | 0.707   |\n",
    "| PR AUC                     | 0.421       |\n",
    "| Accuracy                      | 0.671       |\n",
    "| F1-Score                     | 0.446           |\n",
    "| Precision                      | 0.354           |\n",
    "| Recall                      | 0.602          |\n",
    "| Confusion Matrix                      | TP = 37,067. FP = 16,629. FN = 6,033. TN = 37,067.          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64336d39-3ae6-41c2-ac1c-54570b4e22fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatClassCMNorm.png?raw=true' >\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatClassCMRaw.png?raw=true' >\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatClassCMTuned.png?raw=true' >\n",
    "\n",
    "In comparison to the 0.5, the 0.55 threshold improves precision and accuracy by 2.7 and 5.5, respectively, while displaying a decrease in recall of 10.6. Based on business use case scenario we can choose the desireable threshold, if we want fewer false alarms we should use 0.55 or higher to improve precision and accuracy. If we want to catch more delays we should use 0.5 ot lower for optimal recall. The normalized confusion matrix emphasizes this trade off, 0.55 we correctly identify ~60% of truly delayed flights while only 31% of the actual on-time flights are flagged. \n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatClassMetrThresh.png?raw=true' >\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatClassPR.png?raw=true'>\n",
    "\n",
    "The PR curve here has an AUC of 0.421 which is greater than the random forests' AUC of 0.36 on the same plot. This comes into play since we care more about the performance on the positive class of delay. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatClassROC.png?raw=true'>\n",
    "\n",
    "The ROC curve boasts a solid AUC of 0.707, which is also better class separation than the random forest classifier with an AUC of 0.673. Based on this we can infer that Catboost is learning richer interactions by quantifying the categorical structure of our data, leading to better results. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatClassFeature.png?raw=true'>\n",
    "\n",
    "Again we see the engineered features assisting our model the most, suggesting that simple model prediction of the dataset as it is would not prove useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6887efe-0738-4f73-bfb0-4185c52eab7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Next Steps for CatBoost Classification\n",
    "\n",
    "To close, the Catboost classifier displayed clear improvement over the random forest baseline, with the ability to clearly distinguish between classes and boast improved precision recall tradeoffs for classifying delay. To maximize value for end users, future work would include tailoring thresholds by carrier, airport, or time to align with specific priorities. We can also enrich the feature set with live weather data as suggested for the random forest, and adopt cost sensitive tuning that optimizes the balance between the cost of missed delays and false alarms. Finally, integrating this model into a two stage workflow where catboost classifiered flags high risk flights, and a second stage regression model quantifies the severity of the potential delay would enable the most proactive disruption management. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ddc1f08-34d2-46fd-864b-ee711566e449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## XGBoost Classifier\n",
    "Due to XGBoost classifier inherently able to handle nulls, we allowed the model to learn optimal default directions for missing data during tree construction without requiring explicit imputation. This is particular important for examing our aircraft delays since the missingness may carry predictive information about future delays. Upon examination of the distribution of probabilities for the classifier the default 0.5 to 0.07 to prioritize higher recall, ensuring that a greater proportion of delay cases were detected, even at the expense of marking on time flights as delayed. This threshold tuning was essential given the business context, as an airline we want to capture more of the delays, this came at the expense of approximately 10% of non-delayed flights labeled as delayed. With the optimized hyperparameters explored below, the model achieved an accuracy of 81.11%. While precision (0.4773) and recall (0.5043) were balanced under this setting, the lowered threshold further enhanced sensitivity, aligning the model's output with the improved delay identification\n",
    "\n",
    "\n",
    "| Metric    | Value  |\n",
    "|-----------|--------|\n",
    "| Accuracy  | 0.8111 |\n",
    "| Precision | 0.4773 |\n",
    "| Recall    | 0.5043 |\n",
    "| F1 Score  | 0.4904 |\n",
    "| AUC       | 0.7775 |\n",
    "\n",
    "\n",
    "| max_depth | learning_rate | gamma     | lambda    | alpha     | subsample | colsample_bytree | n_estimators |\n",
    "|-----------|---------------|-----------|-----------|-----------|-----------|------------------|--------------|\n",
    "| 3         | 0.0640641254  | 1.4355975 | 0.4640801 | 0.0140250 | 0.6460691 | 0.6145783        | 184          |\n",
    "\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/roc_curve.png\" width=\"45%\" />\n",
    "  <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/confusion_matrix_xgb.png\" width=\"40.5%\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5c1a6f-9fdb-4bf9-aa13-7aac0c965372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLP Classifier\n",
    "\n",
    "An MLP classifier was also trained and tested on the same data as an experiment. Nulls for numeric fields were imputed with median values. Null categorical values were imputed as \"unknown\". The hidden layer structure and sizes were **[256,128]**, with an output size of 2 to correspond to two classes (delayed or not delayed). This is essentially a softmax approach since sigmoid is not supported by the specific library used to execute this in Spark. The metrics on test data are as follows:\n",
    "\n",
    "- AUC-ROC: 0.4648\n",
    "- AUC-PR: 0.1662\n",
    "- Accuracy: 0.8196\n",
    "- F1: 0.7383\n",
    "- Weighted Precision: 0.6717\n",
    "- Weighted Recall: 0.8196\n",
    "\n",
    "F1 is high with this method although the area under the PR and ROC curves is relatively low, implying lower overall model performance but a threshold that happens to work relatively well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d183bd5-296c-46dd-9315-dd4956f06248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8e45a37-c6d0-4ff4-bc94-ef577343e6e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Baseline Predictor based on Naive Predictions\n",
    "\n",
    "To evaluate the predictive performance of our regression models, simple constant-value baselines were established. These baselines serve as lower-bound references to quantify model improvement. Using the training set, three constant predictors were defined:\n",
    "\tâ¢\tZero baseline â always predicts 0 minutes delay.\n",
    "\tâ¢\tMean baseline â predicts the mean departure delay from the training data.\n",
    "\tâ¢\tMedian baseline â predicts the median departure delay from the training data.\n",
    "\n",
    "Each baseline was applied to both training and test splits, and evaluated using standard regression metrics: MSE, RMSE, MAE, and RÂ². This allows direct comparison between the learned model and trivial predictors. The methodology follows a chronological split to preserve temporal ordering and avoid data leakage, aligning with the nature of time-dependent flight delay data.\n",
    "\n",
    "| Base  | Split | Const  |  MSE  | RMSE |  MAE  |   RÂ²   |\n",
    "|-------|-------|--------|-------|------|-------|--------|\n",
    "| ZERO  | train |   0.00 | 1820.56 | 42.67 |  15.52 | -0.0521 |\n",
    "| ZERO  | test  |   0.00 | 2505.70 | 50.06 |  17.25 | -0.0496 |\n",
    "| MEAN  | train |   9.49 | 1730.45 | 41.60 |  19.74 | -0.0000 |\n",
    "| MEAN  | test  |   9.49 | 2389.16 | 48.88 |  21.46 | -0.0008 |\n",
    "| MEDIAN | train |  -2.00 | 1862.53 | 43.16 |  15.27 | -0.0763 |\n",
    "| MEDIAN | test  |  -2.00 | 2553.24 | 50.53 |  16.96 | -0.0695 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a399f3f-4c8f-45e8-9d67-18ac303db4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For the regression portion of our modeling, we shifted focus from predicting categorical delay classes to directly estimating the numeric departure delay in minutes. This approach allows for more granular predictions and provides flexibility for downstream use cases, such as threshold-based alerts or operational planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f432b6d9-498f-44de-af59-546fa25a7722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b5b802-42b8-409b-895e-8caca49bab68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Random Forest algorithm works through a collection of trees (forest) that are more stable and thoeretically accurate than any of the single trees. Multiple trees are trained at once, so this method can be computationally exhaustive. The set up of this model was configured to 200 trees at a depth of 12, using the log(dep_delay) as our target but exponentiated back to minutes for evaulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f8d9a18-2dcc-4d6c-9951-ca2afe2d431c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key Results for Random Forest Regression\n",
    "\n",
    "Time to run: 3 minutes\n",
    "\n",
    "| Dataset | RMSE   | MAE   | RÂ²      |\n",
    "|---------|--------|-------|---------|\n",
    "| Train   | 59.12  | 20.21 | -0.0205 |\n",
    "| Test    | 61.45  | 21.44 | -0.0268 |\n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/RFRegHex.png?raw=true'>\n",
    "\n",
    "This hexagonal binning(hexbin) plot visualizes the density of points in a scatter plot when we have overlapping data, a common theme found in airline data. Most predictions are congregated near low values, again showing the lack of ability to properly estimate large delays.  \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/RfregMAEHour.png?raw=true'>\n",
    "\n",
    "It is interesting to look at the most error-inducing times of day, where we see MAE highest around midnight to to 8 am in the early morning. This could be due to overnight operations, maintenance delay, and schedule recovery scrambling from the day prior. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/RFRegMaeMonth.png?raw=true'>\n",
    "\n",
    "February and August show the largest errors of the months, showing interesting signs of seasonality that typically wouldn't be thought of as the busiest air travel months. This may be lined to winter weather and summer travel peaks, respectively. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/RFRegFeatureImportance.png?raw=true'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c63466f-0425-4476-be8b-098bc9f37d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Takeaways and Next Steps for Random Forest Regressor\n",
    "\n",
    "The log transformation reduced variance but did not drastically help MAe in the grand scheme of things. Within Random Forest, adding log to dep_delay helped tremendously, but departure delays remained highly stchastic due to our inability to predict these values with great accuracy. Future work would include the integration of a classifier to predict 'major delay' risk then run regression on those flagged flights. This pocketing was done for CatBoost and helped tremendously, so we anticipate similar results here. Further feature engineering to extract key relationships not obvious from initial analysis of the dataset would also help greatly, as the artifical features created were the biggest boost to model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e261e412-efae-4f65-9a57-1018ca249acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## XGBoost Model Training\n",
    "\n",
    " While the Random Forest regressor provided a baseline ensemble method, its performance was limited in capturing complex interactions within our high-dimensional feature set. XGBoost was selected as a more advanced gradient boosting algorithm, capable of sequentially correcting errors from prior trees, incorporating regularization, and handling skewed target distributions. The task is again framed as a regression problem, with the target being departure delay in minutes. In addition, before training, extreme target values were capped at the 99th percentile (and symmetrically for negative delays) to reduce the impact of outliers. The capped target was then transformed using a signed log(1+|x|) function to stabilize variance and handle skewed distributions, with predictions inverse-transformed for metric reporting.\n",
    "\n",
    "Feature processing included:  \n",
    "- Numeric features: imputed with mean values.  \n",
    "- Categorical features: string-indexed for XGBoost.  \n",
    "\n",
    "Model configuration used tuned hyperparameters for tree depth, learning rate, child weight, subsampling, and column sampling, with support for additional parameter overrides. Below are the selected hyperparameters optimized via grid search:\n",
    "\n",
    "| Objective        | Learning Rate | Max Depth | Min Child Weight | Column Subsample (By Tree) | Subsample (Rows) | Tree Method | Boosting Rounds | Early Stopping Rounds |\n",
    "|------------------|---------------|-----------|------------------|----------------------------|------------------|-------------|-----------------|-----------------------|\n",
    "| Square Error     | 0.05          | 4         | 1                | 0.8                        | 0.8              | Hist        | 1000            | 30                    |\n",
    "\n",
    "Training was followed by time-series cross-validation, which uses chronological splits to preserve temporal ordering and avoid data leakage. This approach evaluates candidate parameter sets over multiple folds and selects the configuration with the lowest average RMSE.\n",
    "\n",
    "## XGBoost Regression Results\n",
    "\n",
    "The XGBoost regressor achieved a train RMSE of 41.08 minutes and a test RMSE of 49.39 minutes. MAE values were 14.19 minutes (train) and 15.94 minutes (test), with RÂ² scores close to zero, indicating that while the model reduced absolute error compared to baselines. These results suggest that while XGBoost offers some improvement in error metrics, additional feature engineering or model adjustments may be required to achieve substantial predictive gains.\n",
    "\n",
    "| Dataset | RMSE (min) | MAE | RÂ² |\n",
    "|---------|------------|-----|-----|\n",
    "| Train   | 41.08      | 14.19 | 0.0246 |\n",
    "| Test    | 49.39      | 15.94 | -0.0218 |\n",
    "\n",
    "Analyzing the graph, you can see the model continued to underpredict delays. This is something we will aim to address in the CatBoost approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9615d997-39cd-45d0-97f2-8eddf43ad8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/xgboost_predictedvsactual.png\" width=\"50%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4184dea-e2e8-446c-96bf-034acd01549a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CatBoost Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a74f107-895d-4247-9a9e-526e385ae995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Key Results for CatBoost regression\n",
    "\n",
    "Time to execute: 2 minutes\n",
    "\n",
    "| Dataset | RMSE   | MAE   | RÂ²     |\n",
    "|---------|--------|-------|--------|\n",
    "| Train   | 52.84  | 22.10 | 0.1425 |\n",
    "| Test    | 57.07  | 24.32 | 0.1080 |\n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatBoostRegPredvsActual.png?raw=true'>\n",
    "\n",
    "The predicted vs actual scatter plot shows a major cluster in the bottom left, indicating that the model is well-suited to predict smaller delays but fails to predict the larger ones. The underprediction of large delays is a common theme with our models and typically when working with airline data, since the majority of delays are on the smaller side leading to an imbalanced dataset. This visual indicates that the catboost regressor is good for small to medium delays, but may require oversampling of large delays or pocketing to improve overall performance. Pocketing for this model is discussed below. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatBoostRegresidualsDist.png?raw=true'>\n",
    "\n",
    "The residuals plot furhter supports the underestimation of large delays, shown by the right tail. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatBoostRegressorFeatureImportance.png?raw=true'>\n",
    "\n",
    "The feature importance plot explains how delay prediction is mostly driven by operational context and prior flight performance from the engineered features. With weather and schedule timings and secondary influences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8098b9bd-51ae-4c3f-95c9-a0d88461bcf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CatBoost Regressor with Pocketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "076ca3ae-1e58-4453-b455-0b4f792be1a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key Results for CatBoost Regression with pocketing\n",
    "\n",
    "Time to execute: 8 minutes\n",
    "\n",
    "Pocketed Regression (0â60) â Test (2019)\n",
    "| Metric        | Value   |\n",
    "|---------------|---------|\n",
    "| MAE (pocket)  | 12.87   |\n",
    "| RMSE (pocket) | 21.08   |\n",
    "| RÂ² (pocket)   | -0.0202 |\n",
    "\n",
    "On True Small Delays Only (DEP_DELAY â¤ 60)\n",
    "| Metric       | Value |\n",
    "|--------------|-------|\n",
    "| MAE (small)  | 12.87 |\n",
    "| RMSE (small) | 21.08 |\n",
    "\n",
    "Large Delay Classifier (DEP_DELAY > 60) â Test (2019)\n",
    "| Metric       | Value   |\n",
    "|--------------|---------|\n",
    "| AUC          | 0.7392  |\n",
    "| PR AUC       | 0.2815  |\n",
    "| Precision    | 0.4300  |\n",
    "| Recall       | 0.1621  |\n",
    "| F1           | 0.2354  |\n",
    "| True Large % | 8.73%   |\n",
    "| Pred Large % | 3.29%   |\n",
    "\n",
    "**Confusion Matrix**\n",
    "|      | Predicted Positive | Predicted Negative |\n",
    "|------|--------------------|--------------------|\n",
    "| **Actual Positive** | TP = 974             | FN = 5035           |\n",
    "| **Actual Negative** | FP = 1291            | TN = 61570          |\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatPocketPRThresh.png?raw=true'>\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/CatPocketROC.png?raw=true'>\n",
    "\n",
    "Due to the consistent underestimation of large delays, pocketing was explored to enhance model performance. We deemed delays greater than 1 hour as 'detrimental' and delays under as 'manageable'. This allowed us to predict delay as a contunous variable from 15 - 60, and anything greater than this range would be considered simply as a 'severe delay'. The results were very promising and utlizing this paradigm could prove to be key in future work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c1af38e-cde5-41d8-909e-8c0e40587c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ensemble Model (Random Forest and XGBooost)\n",
    "\n",
    "The ensemble model combines our random forest and XGBoost regressor models to yield a robust and complete output that combines the best of both models. To stabilize distributed training we indexed categoricals, assembled + scaled features, repartitioned training data, and used safe XGBoost settings. We then formed a weighted ensemble:\n",
    "$$\n",
    "\\hat{y}_{ens} = 0.3 \\cdot \\hat{y}_{RF} + 0.7 \\cdot \\hat{y}_{XGB}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9b58349-4e73-474b-97b2-c3671ca2dc47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Results\n",
    "\n",
    "| Dataset | RMSE  | MAE   | RÂ²      |\n",
    "|---------|-------|-------|---------|\n",
    "| Train   | 36.82 | 18.74 | 0.0125  |\n",
    "| Test    | 37.95 | 19.89 | 0.0032  |\n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/ensembleAccvsPred.png?raw=true'>\n",
    "\n",
    "The majority of points cluster near the origin which represent minor delays, and the line is the âperfectâ reference. Our ensemble mostly tracks small delays but underestimates large delays. The loss is dominated by the majority of near zero cases, so the model plays it safe on rare 2â6 hour disruptions.\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/ensembleRFfeatureimport.png?raw=true'>\n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/ensembleRffrfeatureimport.png?raw=true'>\n",
    "\n",
    "We can see plenty of similarities between the feature importance top features of the random forest and XGBoost model. This supports the idea that these models work well in tandem and can be tuned further for even greater results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "859260f0-4242-4853-a608-314fe900ec90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Forward Feeding Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8617f4b8-ee88-4000-8241-b896b0c0623a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Multi-Layer Perceptron Neural Network \n",
    "\n",
    "Multi-layer perceptrons (MLPs) are a type of feedforward neural network built on interconnected nodes organized by layers. They are able to learn and adapt to complex inputs by adjusting weights during training, and proved to be very useful for regression. We used StandardScaler fit on the training data and applied it to the test set since MLPs work best with scaled inputs. The model configuration was built using hidden layer sizes of (64,32) ReLU activation, adam optimizer, and early stopping, overall a small and fast baseline. Despite this, MLPs are computationally exhaustive and this model took a whopping 32 minutes to execute. This delayed tuning and enhancement greatly as the waiting period grew vast when we began to introduce more complex configuration inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42085cd9-1af4-4f83-aa23-32085318a3ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key Results for MLPNN\n",
    "\n",
    "Time to execute: 32 minutes\n",
    "\n",
    "| Dataset | MAE   | RMSE  |\n",
    "|---------|-------|-------|\n",
    "| Train   | 15.97 | 37.07 |\n",
    "| Test    | 17.54 | 43.81 |\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/MLPNNregaccpred.png?raw=true'>\n",
    "\n",
    "The plot fo actual vs predicted show yet again the majority of data sitting the low to moderate delay region, however, this model showed the greatest promise of predicting high delay values without the need to pocket or bin dep_delay values. Only above ~1750 did the model truly fall apart and fail to predict any large values. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/MLPregResidual.png?raw=true'>\n",
    "\n",
    "The residuals distribution is also promising for this model, spiking near 0 with a small right tail. The typical errors in this setup are moderate, but the understimation of large delays is evident by the skew, also displaying the effect of the class imbalance yet again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9f44ef-683e-4326-bdf0-edd3d93e7877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Takeaways and Next Steps for MLPNN\n",
    "\n",
    "With an MAE of ~17-18 minutes on the MLP we have a very useful ranking and magnitude estimate. With delays being defined as 15 minutes or more, the MAE is almost within this range to provide meaning in every airline operations. Future work would include pocketing or potentially mixing this model with another well-performing regressor such as CatBoost. This ensemble will be extremely resource consuming but could prove to be a completely robust predictor with little to no error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15b1c59f-d5d1-4638-a615-0d486fbdd25f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A variety of different options were assessed, from introducing weights that favored specifically delays, to linear scaling, square root, to separated piece-wise functions, however none of the attempts favorably allowed sufficient improvement to fundamentally improve the MLPNN in any meaningful way. Results would often either shift non-delays to being predicted as extreme delay, or atleast cluster all the smaller delays together, but not necessarily predicting delays accurately beyond a certain threshold. Even other initial transformations to delays were considered such as square-root, log \\* log, log \\* sqrt, and many others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f099a0f-7eb7-4af1-a174-b8d08f431be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/condensed.png\" width=\"45%\" />\n",
    "  <img src=\"https://raw.githubusercontent.com/mesighel/mids_w261/main/Media/ND_toD.png\" width=\"45%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb0299ed-b642-45bd-bb2f-21016b517fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Results and Discussion\n",
    "\n",
    "\n",
    "### Final Model Comparisons\n",
    "\n",
    "#### Classification Models\n",
    "\n",
    "| model                            | split   |    auc |   accuracy | f1     | precision   | recall   |\n",
    "|:---------------------------------|:--------|-------:|-----------:|:-------|:------------|:---------|\n",
    "| CatBoost Classifier              | Test    | 0.7070 |     0.6710 | 0.4460 | 0.3540      | 0.6020   |\n",
    "| Deep Learning (Keras/TensorFlow) | Test    | 0.7002 |     0.7933 | 0.1469 |             | 0.0831   |\n",
    "| Logistic Regression              | Test    | 0.7893 |     0.7464 | 0.7444 | 0.7452      | 0.7465   |\n",
    "| Logistic Regression Results      | Test    | 0.6504 |     0.7377 |        |             |          |\n",
    "| Random Forest Classifier         | Test    | 0.8249 |     0.7562 | 0.7573 | 0.7618      | 0.7562   |\n",
    "\n",
    "#### Regression Models\n",
    "\n",
    "| model                   | split   |   rmse |   mae |      r2 |\n",
    "|:------------------------|:--------|-------:|------:|--------:|\n",
    "| ZERO                    | Test    |  50.06 | 17.25 | -0.0496 |\n",
    "| MEAN                    | Test    |  48.88 | 21.46 | -0.0008 |\n",
    "| MEDIAN                  | Test    |  50.53 | 16.96 | -0.0695 |\n",
    "| Random Forest Regressor | Test    |  61.45 | 21.44 | -0.0268 |\n",
    "| XGBoost Regressor       | Test    |  49.39 | 15.94 | -0.0218 |\n",
    "| CatBoost Regressor      | Test    |  57.07 | 24.32 |  0.1080 |\n",
    "| MLP                     | Test    |  43.81 | 17.54.|         |\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The results show clear signal for classification. Random Forest leads with AUC 0.8249 and F1 0.7573, and tuned Logistic Regression follows closely at AUC 0.7893 and F1 0.7444. The CatBoost classifier underperforms these two, and the deep classifierâs high accuracy with very low recall (0.7933 vs 0.0831) confirms that default thresholding on an imbalanced label is not usable without calibration. In short, classification is learnable and robust today using RF or a calibrated LR.\n",
    "\n",
    "Regression tells a different story. Simple baselines anchor RMSE near 49â51 on test. XGBoost improves MAE to 15.94 but posts negative RÂ² (â0.0218), which means squared errors still exceed the variance of the target. CatBoost delivers the only positive RÂ² (0.1080) yet with higher RMSE and MAE, trading typical error for modest variance explanation. The MLP achieves the lowest RMSE (43.81) with MAE 17.54. These patterns are consistent with heavy-tailed misses: average absolute error can drop while a few large residuals keep RÂ² flat or negative.\n",
    "\n",
    "The feature-importance analysis shows signal is mostly driven by turnaround and propagation, not raw weather. The top factors are the time between inbound arrival and scheduled departure, previous departure delay, and a second-leg turnaround proxy, indicating delays cascade across legs when buffers are tight. Recent destination disruptions (cancellations and diversions in the prior 2â6h) rank just below these, showing network conditions at the destination affect outcomes even before pushback. For regression, the same pattern explains tail risk: when propagation and rare disruptions spike, squared errors jump; reducing tail impact will likely require better features for extreme events and airport capacity, not just more model depth.\n",
    "\n",
    "If the decision is binary triage, alerts, or SLA gating, classification is ready for production. Random Forest is the best default, with Logistic Regression as a simpler calibrated fallback. Both models show balanced precision and recall at the reported operating point, which aligns with operational needs. If the decision requires a magnitude forecast, regression can help, but only under explicit constraints. Use the MLP where the KPI is RMSE and downstream processes tolerate occasional large misses. Use CatBoost when stakeholders require non-trivial explained variance, accepting higher typical error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66f3cd80-a786-4cfe-9830-3403cc70b3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "We set out to predict flight delays in time to help operations triage risk and set expectations. This matters because even modest gains in early warning reduce missed connections, crew knock-ons, and customer impact. Our target was a 2-hour pre-departure decision. At T-2 hrs the dominant drivers appear to be inbound delay propagation, turnaround buffers, and very recent destination disruptions. Large, weather-driven or air-traffic-driven delays often crystallize inside this window, which explains why classification is reliable while magnitude prediction remains difficult to capture. These results therefore match the operational reality of the horizon we chose.\n",
    "\n",
    "In practice, teams should pair the model with live reporting and qualitative signals that are not fully captured numerically. Examples include dispatcher or station alerts, gate and crew availability notes, maintenance status, ground holds, and very short-horizon weather updates. This hybrid approach preserves the modelâs early-warning value while accounting for on-the-ground factors that shift quickly in the last two hours.\n",
    "\n",
    "### What we learned\n",
    "\n",
    "Classification worked well. Random Forest led (AUC 0.8249, F1 0.7573) with calibrated Logistic Regression close behind. These models offer balanced precision and recall without complex tuning. Regression was harder. The MLP delivered the lowest RMSE (43.81) but models struggled with explained variance and large outliers; CatBoost was the only regressor with positive RÂ² (0.108). Across models, the strongest features were operational: time between inbound arrival and scheduled departure, previous-leg delay, short-horizon disruption at the destination, and schedule context (departure time and month). Weather added value but ranked below turnaround and propagation.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "\t1.\tBring in live, time-aligned weather and nowcasts at the airport level.\n",
    "\t2.\tFix and audit joins so every feature reflects only what is known at prediction time.\n",
    "  3.  Gather more aircraft data (covering potential technical issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc55873-e8d1-44f3-9844-ab6a76d6495e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a5d47b1-4699-4440-9865-82d65f79e408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Predictions on 2020-2021 Flights\n",
    "\n",
    "The following work represents predicted departure delays for the two \"future\" years 2020 and 2021. Since the model pipeline is trained on data before 2019, data after then is relabeled to 2020 and 2021. These 2 years are unseen categories for our model, meaning the random forest regression model used to rpedict these values maps unseen categories to the same \"other\" bucket, resulting in 2020 and 2021 becoming nearly identical to the model since there was no other way for unsupervised learning to differentiate this information. There are caveats with this, along with potential solutions for how to resolve each. If we use real future data, the model can make proper predictions rather than almost blindly guessing. Given our time frame this was not fully explored. We can also involve proper encoding of the field year, as well as explore fine tuning this model once the previous 2 suggestions are implemented. The following chart is best interpreted as a carrier ranking rather than a true year over year change in predictions. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/20212021predflights.png?raw=true'>\n",
    "\n",
    "| YEAR | OP_UNIQUE_CARRIER | ORIGIN | DEST | Prediction          |\n",
    "|------|-------------------|--------|------|---------------------|\n",
    "| 2020 | DL                | IAH    | LGA  | 19.30548030979348   |\n",
    "| 2020 | DL                | MSP    | IAH  | 12.231843245177158  |\n",
    "| 2020 | AA                | LAX    | JFK  | 9.95144373285823    |\n",
    "| 2020 | DL                | DFW    | LGA  | 128.78196971061556  |\n",
    "| 2020 | DL                | DTW    | DFW  | 11.916664472604632  |\n",
    "| 2020 | AA                | CLT    | PVD  | 9.794548208474952   |\n",
    "| 2020 | AA                | LAX    | JFK  | 2.082153078649944   |\n",
    "| 2020 | EV                | ORD    | EVV  | 10.251982250493006  |\n",
    "| 2020 | OO                | LAX    | SFO  | 10.485835149073     |\n",
    "| 2020 | AA                | HNL    | LAX  | 5.1746234926722     |\n",
    "\n",
    "\n",
    "Time to execute: 3 minutes\n",
    "\n",
    "Evaluation on Simulated 2020 Data\n",
    "\n",
    "RMSE: 58.4060\n",
    "MAE:  25.4484\n",
    "RÂ²:   0.0657\n",
    "\n",
    "Evaluation on Simulated 2021 Data\n",
    "\n",
    "RMSE: 58.4060\n",
    "MAE:  25.4484\n",
    "RÂ²:   0.0657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "130d143c-0cbf-4a19-975d-4f3f488f9760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deep Learning With Keras and TensorFlow\n",
    "\n",
    "TensorFlow is a deep learning framework from Google that provides advanced tooling to building and training neural networks. Keras is a neural networks API designed to facilitate user experience with developing models. We developed a neural network classifier based on these modules to advance our model development and experiment with more modern techniques to capture the intricate nature of our problem better. \n",
    "\n",
    "### Key Results\n",
    "\n",
    "Time to run: 8 minutes\n",
    "\n",
    "| Metric                  | Value   |\n",
    "|-------------------------|---------|\n",
    "| Accuracy                | 0.7933  |\n",
    "| F1 Score                | 0.1469  |\n",
    "| Recall                  | 0.0831  |\n",
    "| ROC AUC                 | 0.7002  |\n",
    "| MAE (hard preds)        | 0.2067  |\n",
    "| MAE (probabilities)     | 0.2973  |\n",
    "\n",
    "In this model setup, categorical features such as OP_UNIQUE_CARRIER, ORIGIN, DEST, and more are vectorized as embeddings, where each gets learned as a low dimensional vector. The model was configured as follows: Dense(192, ReLU, Dropout) -> Dense(96, ReLU, Dropout) -> Sigmoid. Training was conducted via Adam, binary cross-entropy, and ReduceLROnPlateau + EarlyStopping on validation AUC. Thresholds were optimally configured via probability cut that maximized validation accuracy, and then evaluated on the test set. \n",
    "\n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/DeepLearningROC.png?raw=true'>\n",
    "\n",
    "This ROC curve sits well above the diagonal, meaning the model ranks delayed flights with great accuracy and value. 0.7 AUC provides a good separation for delayed and on time flights. \n",
    "\n",
    "<img src = 'https://github.com/mesighel/mids_w261/blob/main/Media/AccOverEpochs.png?raw=true'>\n",
    "\n",
    "Both training and validation accuracy rise from ~0.8 to ~0.82, and the curves are close which indicates minimal overfitting in this model. Binary cross entropy falls steadily on training and validation, resulting in stable training and overall solid classification capabilities from an advanced model setup. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2332c875-e5c6-4e4d-ac50-15c76d7cb901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Code Notebooks:\n",
    "- [Additional Weather EDA - Trends by Season and Region](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/851535786086111?o=4021782157704243)\n",
    "- [Weather Prediction Baseline](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2135364053139827?o=4021782157704243)\n",
    "- [Logistic Regression Model](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182593314?o=4021782157704243)\n",
    "- [Graph Feature Generation](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3631386026268945?o=4021782157704243)\n",
    "- [Preceding Flight Feature Generation](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2135364053139367?o=4021782157704243)\n",
    "- [Final Feature Engineering Pipeline](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3600546862781597?o=4021782157704243)\n",
    "- [Feature Dictionary](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2053462565272270?o=4021782157704243#command/7276915796828527)\n",
    "- [XGBoost Model](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/819275088972366?o=4021782157704243)\n",
    "- [XGBoost Model 2](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/819275088972339?o=4021782157704243)\n",
    "- [XGBoost Model 3](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/819275088972349?o=4021782157704243)\n",
    "- [XGBoost Model 4](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/1942580671399376?o=4021782157704243)\n",
    "- [Elastic Net](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/819275088972306?o=4021782157704243)\n",
    "- [Linear Regression and EDA](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/819275088972313?o=4021782157704243)\n",
    "- [Random Forest](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2135364053139255?o=4021782157704243)\n",
    "- [DF Join Work](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/819275088972279?o=4021782157704243)\n",
    "- [Naive Baselines](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/819275088972625?o=4021782157704243)\n",
    "- [Aircraft Specific: 5 Yr EDA/Engineering](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/851535786086407?o=4021782157704243)\n",
    "- [Aircraft Specific: 1 Yr EDA/Engineering](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/851535786086301?o=4021782157704243)\n",
    "- [Aircraft Specific: 3 Mo EDA/Engineering](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/819275088972073?o=4021782157704243)\n",
    "- [CatBoostPocketing](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182593307?o=4021782157704243)\n",
    "- [RandomForestClassifier](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182593267?o=4021782157704243)\n",
    "- [CatBoostClassifier](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182592971?o=4021782157704243)\n",
    "- [MLPNNClassifier](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182592562?o=4021782157704243)\n",
    "- [MLPNNClassifier - Second Attempt](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182592562?o=4021782157704243)\n",
    "- [RandomForestRegressor](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182592537?o=4021782157704243)\n",
    "- [Ensemble](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182592534?o=4021782157704243)\n",
    "- [MLPNNregressor](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/2431760182592532?o=4021782157704243)\n",
    "- [CatBootregresssor](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3600546862782591?o=4021782157704243)\n",
    "- [20201-2021ExtraCredit](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3600546862780720?o=4021782157704243)\n",
    "- [XGBoost Classifier](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3600546862782818?o=4021782157704243#command/4605218292335409)\n",
    "- [FFNN Explorations](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3600546862783177?o=4021782157704243#command/6326987468981346)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase 3 - Final Report",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}