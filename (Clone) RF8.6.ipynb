{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f65dcf-c29e-4f83-88e5-65e7053d7a2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(f\"dbfs:/FileStore/Group_4_3/Data/Pipeline_Output/OTPW_60M_PARQUET\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62036204-2611-4c7e-a8c9-603b648a74d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC9 RMSE (2019): 58.33\n\uD83D\uDCC9 MAE  (2019): 25.35\n\uD83D\uDCC8 R²   (2019): 0.0683\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg, when, floor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# === Step 0: Cast relevant columns and engineer features ===\n",
    "cast_cols = [\n",
    "    \"DEP_DELAY\", \"PREV_DEP_DELAY\", \"PREV2_DEP_DELAY\", \"PREV2_TIME_BETWEEN_ARR_AND_SCHEDULED_DEP\",\n",
    "    \"TIME_BETWEEN_ARR_AND_SCHEDULED_DEP\", \"CRS_DEP_TIME_MINUTES\", \"TIME_BETWEEN_SCHEDULED_ARR_AND_SCHEDULED_NEXT_DEP\",\n",
    "    \"HOURLYVISIBILITY_2H\", \"HOURLYVISIBILITY_3H\", \"HOURLYVISIBILITY_4H\", \"HOURLYVISIBILITY_5H\", \"HOURLYVISIBILITY_6H\",\n",
    "    \"HOURLYWINDSPEED_2H\", \"HOURLYWINDSPEED_3H\", \"HOURLYWINDSPEED_4H\", \"HOURLYWINDSPEED_5H\", \"HOURLYWINDSPEED_6H\",\n",
    "    \"HOURLYWINDGUSTSPEED_2H\", \"HOURLYWINDGUSTSPEED_3H\", \"HOURLYWINDGUSTSPEED_4H\", \"HOURLYWINDGUSTSPEED_5H\", \"HOURLYWINDGUSTSPEED_6H\",\n",
    "    \"HOURLYPRECIPITATION_2H\", \"HOURLYPRECIPITATION_3H\", \"HOURLYPRECIPITATION_4H\", \"HOURLYPRECIPITATION_5H\", \"HOURLYPRECIPITATION_6H\",\n",
    "    \"HOURLYDRYBULBTEMPERATURE_2H\", \"HOURLYDRYBULBTEMPERATURE_3H\", \"HOURLYDRYBULBTEMPERATURE_4H\", \"HOURLYDRYBULBTEMPERATURE_5H\", \"HOURLYDRYBULBTEMPERATURE_6H\",\n",
    "    \"HOURLYSEALEVELPRESSURE_2H\", \"HOURLYSEALEVELPRESSURE_3H\", \"HOURLYSEALEVELPRESSURE_4H\", \"HOURLYSEALEVELPRESSURE_5H\", \"HOURLYSEALEVELPRESSURE_6H\",\n",
    "    \"ORIGIN_CLOSENESS\", \"TAIL_NUM_USE\", \"ORIGIN_RELATIVE_TRAFFIC\"\n",
    "]\n",
    "\n",
    "df_feat = df\n",
    "for c in cast_cols:\n",
    "    df_feat = df_feat.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "# Feature engineering\n",
    "df_feat = df_feat \\\n",
    "    .withColumn(\"CRS_DEP_X_PREV_DELAY\", col(\"CRS_DEP_TIME_MINUTES\") * col(\"PREV_DEP_DELAY\")) \\\n",
    "    .withColumn(\"ARR_TIME_GAP_X_CRS_DEP\", col(\"TIME_BETWEEN_ARR_AND_SCHEDULED_DEP\") * col(\"CRS_DEP_TIME_MINUTES\")) \\\n",
    "    .withColumn(\"TRIPLE_PRODUCT\", col(\"TIME_BETWEEN_ARR_AND_SCHEDULED_DEP\") * col(\"HOURLYVISIBILITY_2H\") * col(\"ORIGIN_CLOSENESS\")) \\\n",
    "    .withColumn(\"DEP_HOUR\", floor(col(\"CRS_DEP_TIME_MINUTES\") / 60))\n",
    "\n",
    "# === Step 1: Time-based split for encodings ===\n",
    "train_data = df_feat.filter(col(\"YEAR\") < 2019)\n",
    "test_data = df_feat.filter(col(\"YEAR\") == 2019)\n",
    "\n",
    "# === Step 2: Target encodings ===\n",
    "mean_delay_carrier = train_data.groupBy(\"OP_UNIQUE_CARRIER\").agg(avg(\"DEP_DELAY\").alias(\"MEAN_DELAY_PER_CARRIER\"))\n",
    "mean_delay_hour = train_data.groupBy(\"DEP_HOUR\").agg(avg(\"DEP_DELAY\").alias(\"MEAN_DELAY_PER_HOUR\"))\n",
    "mean_delay_dest = train_data.groupBy(\"DEST\").agg(avg(\"DEP_DELAY\").alias(\"MEAN_DELAY_PER_DEST\"))\n",
    "\n",
    "df_joined = df_feat \\\n",
    "    .join(mean_delay_carrier, on=\"OP_UNIQUE_CARRIER\", how=\"left\") \\\n",
    "    .join(mean_delay_hour, on=\"DEP_HOUR\", how=\"left\") \\\n",
    "    .join(mean_delay_dest, on=\"DEST\", how=\"left\")\n",
    "\n",
    "# === Step 3: Rare category flags ===\n",
    "origin_counts = train_data.groupBy(\"ORIGIN\").agg(count(\"*\").alias(\"origin_count\"))\n",
    "dest_counts = train_data.groupBy(\"DEST\").agg(count(\"*\").alias(\"dest_count\"))\n",
    "\n",
    "df_flagged = df_joined \\\n",
    "    .join(origin_counts, on=\"ORIGIN\", how=\"left\") \\\n",
    "    .join(dest_counts, on=\"DEST\", how=\"left\") \\\n",
    "    .withColumn(\"IS_RARE_ORIGIN\", when(col(\"origin_count\") < 50, 1).otherwise(0)) \\\n",
    "    .withColumn(\"IS_RARE_DEST\", when(col(\"dest_count\") < 50, 1).otherwise(0))\n",
    "\n",
    "# === Step 4: Define features ===\n",
    "target = \"DEP_DELAY\"\n",
    "exclude = {target, \"origin_count\", \"dest_count\"}\n",
    "all_features = [c for c in df_flagged.columns if c not in exclude]\n",
    "categorical_cols = [c for c in all_features if dict(df_flagged.dtypes)[c] == \"string\"]\n",
    "numeric_cols = [c for c in all_features if dict(df_flagged.dtypes)[c] != \"string\"]\n",
    "\n",
    "# === Step 5: Index + assemble features ===\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "indexed_features = [c + \"_idx\" for c in categorical_cols] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=indexed_features, outputCol=\"features_unscaled\")\n",
    "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\")\n",
    "\n",
    "# === Step 6: Model and pipeline ===\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target, maxDepth=10, numTrees=100)\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, rf])\n",
    "\n",
    "# === Step 7: Final split ===\n",
    "df_final = df_flagged.dropna(subset=[target] + all_features)\n",
    "train = df_final.filter(col(\"YEAR\") < 2019)\n",
    "test = df_final.filter(col(\"YEAR\") == 2019)\n",
    "\n",
    "# === Step 8: Fit model and evaluate ===\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\")\n",
    "rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "\n",
    "print(f\"\uD83D\uDCC9 RMSE (2019): {rmse:.2f}\")\n",
    "print(f\"\uD83D\uDCC9 MAE  (2019): {mae:.2f}\")\n",
    "print(f\"\uD83D\uDCC8 R²   (2019): {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1b5148-59a6-46dc-8cb9-21c295deecb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC9 RMSE (2019): 87.04\n\uD83D\uDCC9 MAE  (2019): 36.57\n\uD83D\uDCC8 R²   (2019): -0.0751\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg, when, floor, log1p, expm1\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# === Step 0: Cast columns ===\n",
    "cast_cols = [\n",
    "    \"DEP_DELAY\", \"PREV_DEP_DELAY\", \"TIME_BETWEEN_ARR_AND_SCHEDULED_DEP\",\n",
    "    \"CRS_DEP_TIME_MINUTES\", \"HOURLYVISIBILITY_2H\", \"HOURLYVISIBILITY_6H\",\n",
    "    \"HOURLYWINDSPEED_2H\", \"HOURLYWINDSPEED_6H\", \"ORIGIN_CLOSENESS\",\n",
    "    \"TAIL_NUM_USE\", \"ORIGIN_RELATIVE_TRAFFIC\"\n",
    "]\n",
    "\n",
    "df_feat = df\n",
    "for c in cast_cols:\n",
    "    df_feat = df_feat.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "# === Step 1: Log-transform target ===\n",
    "df_feat = df_feat.withColumn(\"DEP_DELAY_LOG\", log1p(col(\"DEP_DELAY\")))\n",
    "\n",
    "# === Step 2: Feature engineering ===\n",
    "df_feat = df_feat \\\n",
    "    .withColumn(\"DEP_HOUR\", floor(col(\"CRS_DEP_TIME_MINUTES\") / 60)) \\\n",
    "    .withColumn(\"CRS_DEP_X_PREV_DELAY\", col(\"CRS_DEP_TIME_MINUTES\") * col(\"PREV_DEP_DELAY\")) \\\n",
    "    .withColumn(\"TRIPLE_PRODUCT\", col(\"TIME_BETWEEN_ARR_AND_SCHEDULED_DEP\") * col(\"HOURLYVISIBILITY_2H\") * col(\"ORIGIN_CLOSENESS\")) \\\n",
    "    .withColumn(\"VISIBILITY_TREND\", col(\"HOURLYVISIBILITY_6H\") - col(\"HOURLYVISIBILITY_2H\")) \\\n",
    "    .withColumn(\"WINDSPEED_TREND\", col(\"HOURLYWINDSPEED_6H\") - col(\"HOURLYWINDSPEED_2H\")) \\\n",
    "    .withColumn(\"IS_PEAK_HOUR\", when((col(\"DEP_HOUR\").between(6, 9)) | (col(\"DEP_HOUR\").between(16, 19)), 1).otherwise(0))\n",
    "\n",
    "# === Step 3: Time-based split for encodings ===\n",
    "train_data = df_feat.filter(col(\"YEAR\") < 2019)\n",
    "test_data = df_feat.filter(col(\"YEAR\") == 2019)\n",
    "\n",
    "# === Step 4: Target encodings ===\n",
    "mean_delay_carrier = train_data.groupBy(\"OP_UNIQUE_CARRIER\").agg(avg(\"DEP_DELAY\").alias(\"MEAN_DELAY_PER_CARRIER\"))\n",
    "mean_delay_hour = train_data.groupBy(\"DEP_HOUR\").agg(avg(\"DEP_DELAY\").alias(\"MEAN_DELAY_PER_HOUR\"))\n",
    "mean_delay_dest = train_data.groupBy(\"DEST\").agg(avg(\"DEP_DELAY\").alias(\"MEAN_DELAY_PER_DEST\"))\n",
    "\n",
    "df_joined = df_feat \\\n",
    "    .join(mean_delay_carrier, on=\"OP_UNIQUE_CARRIER\", how=\"left\") \\\n",
    "    .join(mean_delay_hour, on=\"DEP_HOUR\", how=\"left\") \\\n",
    "    .join(mean_delay_dest, on=\"DEST\", how=\"left\")\n",
    "\n",
    "# === Step 5: Rare category flags ===\n",
    "origin_counts = train_data.groupBy(\"ORIGIN\").agg(count(\"*\").alias(\"origin_count\"))\n",
    "dest_counts = train_data.groupBy(\"DEST\").agg(count(\"*\").alias(\"dest_count\"))\n",
    "\n",
    "df_flagged = df_joined \\\n",
    "    .join(origin_counts, on=\"ORIGIN\", how=\"left\") \\\n",
    "    .join(dest_counts, on=\"DEST\", how=\"left\") \\\n",
    "    .withColumn(\"IS_RARE_ORIGIN\", when(col(\"origin_count\") < 50, 1).otherwise(0)) \\\n",
    "    .withColumn(\"IS_RARE_DEST\", when(col(\"dest_count\") < 50, 1).otherwise(0))\n",
    "\n",
    "# === Step 6: Feature selection ===\n",
    "target = \"DEP_DELAY_LOG\"\n",
    "exclude = {\"DEP_DELAY\", target, \"origin_count\", \"dest_count\"}\n",
    "all_features = [c for c in df_flagged.columns if c not in exclude]\n",
    "categorical_cols = [c for c in all_features if dict(df_flagged.dtypes)[c] == \"string\"]\n",
    "numeric_cols = [c for c in all_features if dict(df_flagged.dtypes)[c] != \"string\"]\n",
    "\n",
    "# === Step 7: Index + assemble features ===\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "indexed_features = [c + \"_idx\" for c in categorical_cols] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=indexed_features, outputCol=\"features_unscaled\")\n",
    "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\")\n",
    "\n",
    "# === Step 8: Random Forest model ===\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target)\n",
    "\n",
    "# === Step 9: Build pipeline ===\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, rf])\n",
    "\n",
    "# === Step 10: Prepare training/testing data ===\n",
    "df_final = df_flagged.dropna(subset=[target] + all_features)\n",
    "train = df_final.filter(col(\"YEAR\") < 2019)\n",
    "test = df_final.filter(col(\"YEAR\") == 2019)\n",
    "\n",
    "# === Step 11: Hyperparameter tuning ===\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(rf.maxDepth, [10, 15])\n",
    "    .addGrid(rf.numTrees, [100, 150])\n",
    "    .build())\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# === Step 12: Fit and evaluate ===\n",
    "cv_model = cv.fit(train)\n",
    "predictions = cv_model.transform(test)\n",
    "\n",
    "# Inverse transform the log prediction for reporting\n",
    "predictions = predictions.withColumn(\"prediction_exp\", expm1(col(\"prediction\")))\n",
    "predictions = predictions.withColumn(\"DEP_DELAY\", col(\"DEP_DELAY\").cast(\"double\"))\n",
    "\n",
    "# === Step 13: Final evaluation ===\n",
    "final_evaluator = RegressionEvaluator(labelCol=\"DEP_DELAY\", predictionCol=\"prediction_exp\")\n",
    "rmse = final_evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "mae = final_evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
    "r2 = final_evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "\n",
    "print(f\"\uD83D\uDCC9 RMSE (2019): {rmse:.2f}\")\n",
    "print(f\"\uD83D\uDCC9 MAE  (2019): {mae:.2f}\")\n",
    "print(f\"\uD83D\uDCC8 R²   (2019): {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39920329-edd0-41fc-89b0-7645562329df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) RF8.6",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}